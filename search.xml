<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[cs224n-papers]]></title>
    <url>%2F2020%2F07%2F28%2Fcs224n-papers%2F</url>
    <content type="text"><![CDATA[This post will mainly contain the paper notes about the papers in cs224n course. Usally some typical papers in nlp field. word2vecModel ArchitecturesMainly two types: NNLM and RNNLM. New log-linear modelsCBOWCBOW(continuos bag of words) model, the main idea is using context to predict the center word. Input is the context words, with one projection hidden layer, and output center word. The architecture is shown: Skip-gram modelSkip-gram model is different from CBOW, in skip-gram model, it tries to maximize classification of a word based on another word in the same sentence. So input is the center word, with one projection layer, and output is the probabilities of other context words. The architecture is shown: Distributed Representations of Words and Phrases and their CompositionalityAbstractIn this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. Skip-gram modelTry to maximize the avg log probility: $$\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)$$ And the probility is calculated using softmax function: $$p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w o}^{\prime}{\top} v_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime}{ }^{\top} v_{w_{I}}\right)}$$ But in actual scene, it’s inpractical because the $\nabla \log p\left(w_{O} \mid w_{I}\right)$ is related to W, usally is very large, so there are some tricks to faster the training process. Hierarchical SoftmaxUsing Huffleman tree to reduce the word num in softmax function. Negative Samplingnegative sampling has 2 effects that can fast the training process. First is to use $\sigma$: logisitic regression to replace the softmax to represent the probility. Second is to subsample the negative samples. Meanwhile it can just update some of the parameters in backpropogation, because we just random choose k negative samples not all negative samples. The formula can represent as: $$\log \sigma\left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)+\sum_{i=1}^{k} E_{w_{i} \sim P_{n}(w)}\left[\log \sigma\left(-v_{w_{i}}^{\prime}{ }^{\top} v_{w_{I}}\right)\right]$$ To replace the $\log P\left(w_{O} \mid w_{I}\right)$ term in skip-gram. Subsampling of Frequent WordsTo counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word $\omega_i$ in the training set is discarded with probability computed by the formula: $$P\left(w_{i}\right)=1-\sqrt{\frac{t}{f\left(w_{i}\right)}}$$ t is a sample threshold, t is higher means the probility of discarding will go high. And $f(\omega_i)$ means the frequency of word $\omega_i$.]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>cs224</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data-processing]]></title>
    <url>%2F2019%2F07%2F16%2Fdata-processing%2F</url>
    <content type="text"><![CDATA[这是本次项目实训课程当中，我所负责的数据后处理模块所实际使用的两种算法。一种是simhash算法，Google在用的海量网页去重的算法，另一种是textrank算法，用于进行关键词提取、文档自动摘要等算法。在这里对这两种算法进行相关的阐述。 课程论文简述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 因为在本次实现的搜索引擎项目当中，我主要负责的就是各类数据的获取，以及主要的数据后处理工作。爬虫部分感觉没有什么好讲的，通过scrapy和selenium、requests, 对于一般的网站爬取工作都可以胜任，不过为了应对很多网站的反爬虫策略，我们自己实现的IP池还是有亮点的，不过不是我写的所以我这里就不展开。我觉得比较有特色是数据后处理部分，在得到数据之后必要的工作是需要对数据进行清洗和加工，所以我主要选取了项目当中在用的simhash文本相似度过滤算法和textrank自动摘要算法来写本次的课程论文。 文本相似度处理 文本距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在正式介绍文本相似度算法之前，首先我会对几种比较常用的文本距离计算方式进行介绍。因为文本相似度计算的基础还是在于不同文本当中的文本距离是如何定义的，不同的文本距离计算会导致不同的相似度的生成。 欧几里得距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 欧几里得距离又称欧氏距离。是我们实际当中使用非常频繁的一种距离计算方式。在欧氏空间当中，两个点$x=(x_1, x_2, …, x_n), y=(y_1, y_2, …, y_n)$之间的欧氏距离可以表示为: $$d(x, y) = \sqrt{\sum_{i=1}^n(x_i - y_i)^2}$$ 曼哈顿距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 曼哈顿距离是用来在欧氏空间标明两个点在标准坐标系上的绝对轴距总和的一种距离度量方式。从几何的角度来看，欧氏距离度量的是两个点的实际距离，曼哈顿距离度量的是两个点的绝对轴之间的距离。例如在直角坐标系当中$x=(x_1, x_2), y=(y_1, y_2)$, 那么两个点之间的曼哈顿距离表示为： $$d(x, y) = |x_1-y_1| + |x_2 - y_2|$$ 余弦距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 余弦距离 是通过计算两个向量之间夹角余弦值来衡量两个向量之间相似度的一种常用的方法。我们都知道如果两个向量之间的夹角越小，余弦值就越接近1.所以我们可以通过计算两个向量的余弦夹角来衡量两个向量之间的相似度。给定两个向量$A, B$， 他们余弦距离衡量公式如下： $$similarity(A, B) = \cos(A, B) = \frac{\sum_{i=1}^n A_i \cdot B_i}{\sum_{i=1}^n(A_i)^2 \cdot \sum_{i=1}^n(B_i)^2}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果similarity越接近1，我们认为这两个向量之间的距离越小，也即相似度也就越高。 海明距离(Hamming distance) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 海明距离又称Hamming distance. 是在编码领域常用的一种距离度量的方法。而且方法和思想非常的朴素，相信大家之前在计算机网络课程当中已经接触过海明码的概念。海明距离对于二进制的字符串之间的计算，非常简单，就是直接计算所有位上，不同的码字的数量。例如1011101与1001001之间的汉明距离是2. 虽然hamming distance的思路非常简单，但是它在很多领域当中都有着非常重要的作用，在之后我会介绍的，在我们搜索引擎项目当中使用的文本相似度算法就是利用了hamming distance来进行计算相似度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 同时因为hamming distance对于0，1字符串的度量有非常快速的算法，之间通过位运算的方式可以快速的求得两个0, 1字符串之间的海明距离，在我们后处理当中，就是利用这种方法能够快速进行hamming distance计算： 123456789101112131415'''params: code_1 -&gt; str: first hamming code code_2 -&gt; str: second hamming code hash_bits -&gt; int: number of the length of two hamming codereturn: ans -&gt; int: hamming distance of two hamming code'''def HammingDistance(self, code_1, code_2): x = (code_1 ^ code_2) &amp; ((1 &lt;&lt; self.hash_bits) - 1) ans = 0 while x: ans += 1 x &amp;= x - 1 return ans 海量文本相似度算法Simhash &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在这次的搜索引擎项目当中，数据后处理的首先问题就是如何对收集到的各种信息进行去重。一个最直接的方法就是根据URL使用set进行去重，当然这只是最基本的重复滤重。还有另外一种情况我们也必须要考虑到，也就是存在文章转载或者抄袭的情况，所以我们对于这种情况也必须进行数据的处理和滤重。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一般情况下，如果要计算两篇文档之间的相似度，传统的思路可能是： 可以通过将两篇文章进行分词，通过关键词和对应的权重生成特征词向量，然后就能通过我们前面介绍的几种文本距离计算的方法进行距离度量（比如可以计算特征向量之间的欧氏距离，或者是通过余弦距离来度量两个特征词向量之间的相似度）。通过距离的大小来度量两篇文档的相似度。 通过hash的方法，为每一篇文档通过哈希的方法生成一个特定的哈希指纹，然后通过比对生成的哈希指纹对文本的相似度进行计算。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 当然这两种方法都存在着很大的弊端，对于第一种方法的话，在海量的数据面前，这个算法所需要耗费的时间是难以想象的，所以这种方法运用到搜索引擎上，理论上在时间的花费成本非常之高。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 对于第二种方法的话，重点就在于如何设计一个好的hash函数，如果采用传统的hash方法，比如说是md5这种局部不敏感的哈希方法的话，那么在文本有很小的变动时候，生成的md5哈希码的差异也会相差甚远，显然这不是我们需要的hash函数。我们理想的hash函数应该对于相似的文档能够生成比较相似的哈希码，从而我们才能从hash code当中提取文章的相似度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 所以接下来就会介绍Simhash的算法，用来解决巨大数据量网页去重的算法。该算法最初是Google提出的一种用于网页去重的算法，详细的论文可以参考《detecting near-duplicates for web crawling》。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Simhash是局部不敏感哈希方法当中的一种。也即输入文档的小部分改动对最后生成的hash code并不会有很大的影响。Simhash方法的主要思想是降维思想，通过将高维的特征词向量降维到生成的hashcode维度，一般选取的64bits或者128bits生成hashcode. 然后通过计算不同文档之间simhash值的hamming distance来度量不同文档之间的相似度。具体的simhash算法一般处理为六个步骤：分词 -&gt; hash -&gt; 加权 -&gt; 合并 -&gt; 降维 -&gt; hamming distance. 分词 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 分词步骤当中主要是对文档进行分词，并且确定每个分词的权重分布。中文分词我所使用的是jieba进行分词，因为我们项目当中大部分还是收集的中文博客、demo等资源，并且在实际测试的时候发现jieba对于英文的分词效果也还是可以的！其中权重值在后续会使用，代表的是该词语在整篇文章当中的重要性。 hash &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hash步骤主要是对上面得到的文档词语进行哈希函数的计算，通常在这里需要指定hash_bits, 也即后续的每篇文档都会降维到hash_bits这个维度上。通过哈希函数处理之后，每个字符串都会变成一个01的字符串，同时长度为我们所希望的hash_bits. 加权 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 加权过程需要使用到上述我们得到的权重值，通过对所有得到的hash值乘上对应的权重，得到对应的加权后的新哈希值。其中因为得到的是01字符串，我们规定遇到1就是正值，遇到0就是负值，而后进行计算。举个🌰，比如浙江大学编码为101010, 并且权重为5的话，通过加权之后，得到的新的加权哈希值就为：5 -5 5 -5 5 -5. 其他的分词后的词语类似处理。 合并 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 通过上面加权步骤之后，我们得到了所有分词的加权hash code，之后我们对所有的这些加权hash code进行相加，相加之后得到的加权hash code就做为整篇文档的加权hash code. 再举个🌰，我们得到浙江大学哈希值为5 -5 5 -5 5 -5, 同时我们还得到软件工程这个词的哈希值为4 4 -4 4 -4 4, 那么总的加权的哈希值为9 -1 1 -1 1 -1. 其他的处理流程都是类似的。 降维 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 降维步骤就是对上述得到的加权code进行一个简单的Sign计算，也即码值大于0的记为1，其他的记为0，就比如上面得到的那一串加权的hash code，在经过降维处理之后，我们就能得到最后文章生成的01 hash code为：101010. Hamming distance &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在上述的pipelines之后我们得到了整篇文档的hash code，接着只需要对两篇文档的hash code计算hamming distance，就能很快的得到两篇文档之间的相似度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 整个Simhash算法的流程可以用下面这张图进行展示，很清晰的展示了整个算法的流程和做法： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 对于Simhash算法在实际处理海量数据时候还有很多其他的优化查询方式，比如对最后生成的hash code进行分块查询，建立倒排索引等方式加快在样本数据库当中的搜索查询速度。我们项目当中的数据数量远不及Google上的样本数量，所以并没有进行过多的优化，直接计算hamming code进行判断。在项目的后处理模块当中，我将simhash算法封装为一个类，其中使用了开源项目simhash来完成hash code的计算。并且当相似度超过95%的时候，我就认为这两篇文档是非常相似的，需要从我们的数据库当中过滤掉，具体实现可以参考我们搜索引擎项目当中的实现SimHashFilter. 实际效果测试 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在实现了simhash算法之后，我对实现的该算法进行了相关测试和性能估计。在项目当中我所用的hash_bits长度为128, 因为实际测试时候使用64bits的hamming code精度可能不太符合实际的需求，所以我提升到了128位的hash_bits. 测试代码和测试文本如下： 123456789101112131415from common.SimHash import SimHashFiltertext1 = "沉默螺旋模式中呈现出民意动力的来源在于人类有害怕孤立的弱点，但光害怕孤立不至于影响民意的形成，" \ + "主要是当个人觉察到自己对某论题的意见与环境中的强势意见一致（或不一致时），害怕孤立这个变项才会产生作用。" \ + "从心理学的范畴来看，社会中的强势意见越来越强，甚至比实际情形还强，弱势意见越来越弱，甚至比实际情形还弱，\这种动力运作的过程成–螺旋状"text2 = "沉默螺旋模式中呈现出民意动力的来源在于人类有害怕孤立的弱点，但光害怕孤立不至于影响民意的形成，" \ + "从心理学的范畴来看，社会中的强势意见越来越强，甚至比实际情形还强，弱势意见越来越弱，甚至比实际情形还弱，\这种动力运作的过程成–螺旋状" \+ "主要是当个人觉察到自己对某论题的意见与环境中的强势意见一致（或不一致时），害怕孤立这个变项才会产生作用。"filter = SimHashFilter(128)print(bin(filter.GetCodeForText(text1)))print(bin(filter.GetCodeForText(text2)))print(filter.IsSimilarByText(text1, text2)) 文本1: &quot;沉默螺旋模式中呈现出民意动力的来源在于人类有害怕孤立的弱点，但光害怕孤立不至于影响民意的形成，&quot; + &quot;主要是当个人觉察到自己对某论题的意见与环境中的强势意见一致（或不一致时），害怕孤立这个变项才会产生作用。&quot; + &quot;从心理学的范畴来看，社会中的强势意见越来越强，甚至比实际情形还强，弱势意见越来越弱，甚至比实际情形还弱， 这种动力运作的过程成–螺旋状&quot; 文本2: &quot;沉默螺旋模式中呈现出民意动力的来源在于人类有害怕孤立的弱点，但光害怕孤立不至于影响民意的形成，&quot; + &quot;从心理学的范畴来看，社会中的强势意见越来越强，甚至比实际情形还强，弱势意见越来越弱，甚至比实际情形还弱， 这种动力运作的过程成–螺旋状&quot; + &quot;主要是当个人觉察到自己对某论题的意见与环境中的强势意见一致（或不一致时），害怕孤立这个变项才会产生作用。&quot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 运行结果如下，可以看到生成的128位的hash code是完全相同的，上述的文本调换了语序生成的两段文本，相似度为1，所以这两段文本会被直接过滤掉其中一段文本。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在实际对后台数据库当中的数据运行该后处理模块时候，可以很明显的看到CSDN当中爬取到的博客文章被删掉了一半左右，因为真的很多转载的和抄袭的博客文章在其中，而对于stackoverflow这种大型的正规的QA网站，基本没有过滤掉很多是因为相似度超过阈值的这种文章，技术水平还是可以的，至少平时在看CSDN里的技术博客都会有这种感受，真的相似或者完全相同的文章太多了！！！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 另外需要提一点的就是simhash算法对于短文本的表现并不是很好，因为短文本进行分词之后，词语本身就很少，所以很大程度的影响了后面的hashing过程。不过在我们这次的项目当中都是内容比较多的文本，所以simhash算法的效果还是挺不错的 :） 摘要生成算法TextRank&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TextRank是一种基于无向图的无监督算法，可以用来进行关键字提取与文章摘要生成等工作。TextRank算法的前身是Google为网页排序而提出的PageRank的算法。PageRank的主要思想也是基于图论的方法，用网页之间的链接来形成庞大的有向图，图中的边权重代表的是从用户从某个网页进入到另一个网页的概率，然后进行迭代更新，得到rank最大的那些网站显示。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TextRank的思想和PageRank很像，算法的主要流程由如下这几点组成： 文本分割为句子 对分割出来的句子进行向量化,其中也包括了stopwords、词性的过滤步骤 确定不同句子之间是否存在边，这个利用的是类似滑动窗口的方法，在固定长度K的窗口当中，如果共同出现n个单词，我们就认为这两个句子之间存在边。一般情况下n取2. 计算不同句子之间的相似度，用句子之间的相似度充当无向图当中边的权重 构造相似度矩阵 构造无向图，其中节点是每一个分割出来的句子，每条边的权重是不同句子之间的相似度 迭代更新TextRank的值，直到最后无向图中节点的TextRank值收敛 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 对于每个句子节点的TextRank值的计算公式如下： $$TR(V_i)=(1-d)+d*\sum_{V_j\in In(V_i)} \frac{\omega_{ji}}{\sum_{V_k \in Out(V_j)} \omega_{jk}} * TR(V_j)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 其中变量$d$表示的是阻尼系数，主要是为了避免有的节点和别的节点之间没有边而出现的TextRank值为0的情况。$\omega_{ik}$代表的就是$i, k$两个句子节点之间的权重值。之后只要根据更新公式进行迭代计算直至收敛即可，然后选取TextRank值最高的几个节点对应的句子，就能形成整篇文档的摘要，实现了自动摘要生成的功能。具体的流程如下所示： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 不过稍微有些遗憾的是，我并没有在我们的项目当中再造轮子实现TextRank算法，要是时间能再长一点可以尝试着实现一波TextRank算法。在我们项目当中使用的是一个开源项目textrank4zh, 这个项目使用的自动摘要算法就是使用的TextRank的算法实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 当然这种方法的速度比较慢，毕竟需要针对全文构建一张无向图，然后在无向图上进行迭代更新，所以实际运行的时候，很大一部分时间是花费在自动摘要生成上面，大概一篇文章2秒左右的生成时间，所以我们花了一晚上的时间对所有收集到的文章进行自动摘要。当然我们也不能保证TextRank就能生成最准确的摘要信息，所以以后有机会去调研其他的摘要生成、关键词生成的算法和研究。 小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在这篇小论文当中，我主要是阐述了本次项目当中实际运用到的simhash和textrank两个算法。当然其实这两个算法都有一定的年头了，在实际的效果上比不上现在大热的NLP技术。不过这次项目能够让我在短时间内完成整个数据获取和数据清洗加工的过程，并且去了解相关的算法研究，我觉得还是有一定的价值和意义的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 之后如果有时间的话，可以对其他的相似度计算和摘要生成、关键字生成的算法进行调研、研究，相信对之后的科研生活有一定帮助的。 参考文献 TextRank算法的基本原理及textrank4zh使用实例 使用TextRank算法为文本生成关键字和摘要 Detecting Near-Duplicates for Web Crawling Similarity estimation techniques from rounding algorithms simhash算法]]></content>
      <categories>
        <category>data-processing</category>
      </categories>
      <tags>
        <tag>data-processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine-learning-4-Kernel-Function]]></title>
    <url>%2F2019%2F07%2F06%2Fmachine-learning-4-Kernel-Function%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在前面几篇文章当中介绍的Linear model, logistic regression, SVM这几种supervised learning的分类器，但是他们只能handle linear的情况，对于非线性可分的情况就爱莫能助了，所以这里主要介绍kernel核方法来讨论non-linear的情况。 Generalized Linear Function&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 当一组数据在当前维度下线性不可分，我们可以对这个维度的数据进行mapping，也即映射到更高维的数据，在高维的空间当中就可能是线性可分的，也就可以利用我们之间介绍的一系列的方法，下面是一个mapping的例子🌰： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 下面我们介绍一下什么叫做Generalized Linear Function, 简单来说就是在linear model的基础之上，增加相关变量。也即在linear model当中我们有$[x_1, x_2, x_3]$这三个featrue，我们增加与三个feature相关的变量，也即扩展到$[x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3]$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 接下来就是在这个新的featrue space去学一个线性的分类器即可。 Quadratic Discriminant Function$$g(x) = \omega_0 + \sum_{i=1}^d\omega_ix_i + \sum_{i=1}^d \sum_{j=1}^d \omega_{ij}x_ix_j$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $g(x) = 0$ 代表的就是一个超平面，在新的映射的featrue space里面的超平面，将数据分割成两类。 Generalized Discriminant Function$$g(x) = \sum_{i=1}^{\hat{d}}a_iy_i(x) = a^Ty$$ $$\begin{aligned}a = [a_1, a_2, …, a_d]^T &amp;&amp; y = [y_1(x), y_2(x), …, y_d(x)]^T\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 其中的mapping y叫做augmented featrue vector, 或者叫做phi function. Representer Theorm&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我们后面也介绍的kernel function都是建立在Representer Theorm的基础之上。这里我只简单的介绍一下形式，具体形式可以查看链接当中PDF的详细介绍。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 简而言之，这个定理描述的就是，任何形如 loss function + Reguralizer的函数都能这样被表示： $$f(x) = \sum_{i=1}^n\alpha_ik(x_i, x)$$ Kernelized Ridge Regression$$\begin{aligned}\omega^* &amp;= argmin\sum_{i=1}^n(y_i-x_i^T\omega)^2 + \lambda\sum_{j=1}^p \omega_j^2 \\&amp;= (XX^T + \lambda I)^{-1}Xy \\&amp;= X(X^TX+\lambda I)^{-1}y \\\alpha &amp;= (X^TX+\lambda I)^{-1}y \\\omega^* &amp;= X\alpha = \sum_{i=1}^n \alpha_ix_i\end{aligned}$$ $$f(x) = \omega^Tx = \sum_{i=1}^n\alpha_ix_i^Tx = \sum_{i=1}^n \kappa(x_i, x)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 其中的$\kappa(x_i, x)$就是ridge regression的kernel function，只要来一个x，只需要放到这个kernel function当中计算一下就fine了。 Kernels&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Let $\kappa(x_i, x) \ge 0$ be some measure of similarity of $(x_i, x)$, we will call $\kappa$ a kernel function. 一般来说$\kappa$都是对称的，并且是positive的，下面是一些常用的kernel function： linear kernel: $\kappa(x, x’) = x^Tx_i$ Polinominal kernel: $\kappa(x, x’) = (x^Tx’ + 1)^d$ RBF kernel: $\kappa(x, x’) = exp(-\frac{||x-x’||^2}{2\sigma^2})$ Advantages Non-linear function Works when the samples can’t represent as featrue vectors String kernels, graph kernels, etc.]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine-learning-3-Linear-Classifier]]></title>
    <url>%2F2019%2F07%2F05%2Fmachine-learning-3-Linear-Classifier%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在前一篇文章当中，我们详细的阐述了Linear model的表现，不过它主要做的是supervised learning当中的一个regression问题，那还剩下另外一个比较重要的问题：classification.所以在这一篇文章当中，会主要介绍logistic model和SVM两种linear classifier. Sigmoid function (logistic function)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sigmoid function 是 logistic regression当中最重要的一个元函数。我们也叫它S型函数： $$\sigma(t) = \frac{e^t}{1+e^t} = \frac{1}{1+e^{-t}}$$ Logistic Regression&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 逻辑斯蒂回归是一种classification的方法，利用的就是sigmoid function来进行classify. Logistic Regression的主要形式如下： $$p(y_i=\pm1|x_i, a) = \sigma(y_ia^Tx_i) = \frac{1}{1+e^{-y_ia^Tx_i}}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 然后对于数据我们同样使用的MLE的方法来估计参数$a$. $$l(p(D)) = -\sum_{i \in I}\log(1+e^{-y_ia^Tx_i})$$ $$E(a) = \sum_{i \in I}\log(1+e^{-y_ia^Tx_i})$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 但是注意的是$E(a)$是一个non-convex function对于参数$a$而言。换言之这里直接求导是没用的，所以我们引入一个新的求参数最值的方法：Gradient descent Gradient descent first order optimization problem (通过求解一阶导可以求解) Can find a local optimum 梯度相反方向：local minimum, 梯度方向：local maximum $$\begin{aligned}\omega_{n+1} = \omega_n - \gamma \nabla J(\omega_n) &amp;&amp; n \ge 0\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 其实这只是一阶导下面的梯度下降的方法，我们来看一看为什么可以进行梯度下降，或者说梯度下降为什么会work呢？ $$E(a+\triangle a) = E(a) + E’ (a)\triangle a + E’’ (a)\frac{(\triangle a)^2}{2!} + …$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这是大家很熟悉的拉格朗日展开，如果我们只取前两项进行估计，则有$E(a)+E’ (a)\triangle a \implies \triangle a = -\eta E’ (a)$. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果是采用前三项进行估计，那么这个叫做牛顿法估计，那么在这种估计之下我们会有： $$E’ (a)\triangle a + E’’ (a)\frac{(\triangle a)^2}{2!} \implies E’ (a)+E’’ (a)\triangle a = 0 \implies \triangle a = -\frac{E’ (a)}{E’’ (a)}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 对于牛顿法求梯度下降更通用的写法如下所示，本质也就是上面式子推导而来： $$\triangle a = -\eta [HE(a)]^{-1} E’ (a)$$ Regularized Logistic Regression L2-Regularizer L1-Regularizer (Sparse Logistic Regression) $$E(a) = \sum_{i \in I}\log(1+e^{-y_ia^Tx_i}) + \lambda\sum_{j=1}^p(a_j^2)$$ SVM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我们知道，对于一个线性可分的数据集，他的solution vector $a$ 可以有无数条，只要能够将两种数据给分开就行。那么我们就想，这么多个solution vector当中，哪条是最优的呢？这就引出今天的另一位主角，就是SVM-solution vector machine, 也称为large margin classifier. Geometrical Margin &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 已知solution vector $\omega$, 如何计算任意一个点x到分割面的距离呢： $$x = x_0+\frac{y\gamma\omega}{||\omega||}$$ $$\omega^Tx = \omega^T(x_0+\frac{y\gamma\omega}{||\omega||}) + b = 0$$ $$\implies \gamma = y\frac{\omega^Tx+b}{||\omega||}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 其中的$\gamma$就被称为Geometrical Margin, 从上面的式子当中我们很容易看出来其实$|f(x)|$就可以衡量不同点到分割面之间的距离。 Margin of dataset &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我们把一个数据集的Margin规定为数据集当中到分割面最小的$\gamma$称为数据集的Margin. 然后我们的target同理也可以变为：Find the hyperplane with the largest margin. 同时我们把那些决定hyperplane的点称为support vectors, 因为远的点对dataset margin没有任何影响，而只有那些组成dataset margin的点才是我们需要关注的点。 $$max_{\omega, b}\gamma = max_{\omega, b}\frac{y(\omega^Tx+b)}{||\omega||}(\gamma_i \ge \gamma)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 因为在这里$\omega$是可以进行任意缩放的，并不会对hyperlane造成任何的影响，所以做一个处理，Fix $y(\omega^Tx+b) = 1 \implies \gamma ||\omega|| = 1$ $$max_{\omega, b}\frac{y(\omega^Tx+b)}{||\omega||} = max\frac{1}{||\omega||}$$ $$\gamma_i = \frac{y_i(\omega^Tx_i+b)}{||\omega||} \ge \gamma \implies y_i(\omega^Tx_i+b) \ge 1$$ $$max\frac{1}{||\omega||} \implies min\frac{1}{2}||\omega^2||$$ $$\begin{aligned}min\frac{1}{2}||\omega^2|| &amp;&amp; s.t. &amp;&amp; y_i(\omega^Tx_i+b) \ge 1\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 所以我们最后的优化目标就变成了上面这个式子了！但是这个式子有一个很明显的weakness：容易受到噪声noise的影响，因为是不允许任何出错的情况的，我们可以看一下如果有一个分类错误的，会影响整个hyperlane的选取： Slack variables &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 为了解决上面的问题，我们引入一个slack variable, 即这个时候我们允许一些犯错，所以我们的新的objective function为： $$min_{\omega, b}\frac{1}{2}||\omega^2||+c\sum_{i=1}^n\xi_i$$ $$\begin{aligned}y_i(\omega^Tx_i+b) \ge 1-\xi_i &amp;&amp; \xi_i \ge 0 \implies \xi_i = max[0, 1-y_i(\omega^Tx_i+b)]\end{aligned}$$ $$E(\omega) = \sum_{i=1}^nmax[0, y_i(\omega^Tx_i+b)] + \frac{1}{2c}||\omega^2||$$ General formulation of classifiers &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 不知道有没有发现，我们之前阐述的objective function都有一个共性，也即Loss function + Regularizer, 即$min\sum_{i=1}^nl(f) + \lambda R(f)$. 所以我们可以总结出这几种loss： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; square loss: $l = (1-yf)^2$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; logistic loss: $l = log(1+e^{-yf})^2$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hinge loss: $l = max[0, 1-yf]$]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine-learning-2-Linear-Regression]]></title>
    <url>%2F2019%2F07%2F05%2Fmachine-learning-2-Linear-Regression%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在前一篇文章当中，我们详细的讨论了Bayes分类器，使用的是后验概率$p(\omega_i|x)$作为它的discriminant function. 那既然监督学习的目标就是学一个mapping，Bayes的方法虽然最优，但并不是直接从data当中得到，所以今天我们将主要介绍Linear model. Linear methods for Regression Linear model &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Linear model 的表达形式很简单，就是类似于平时我们见到的一次方程，即从x到y的mapping函数为： $$\begin{aligned}f(x) &amp;= \omega^TX \\\omega \in R^{d+1} &amp;= [\omega_1, \omega_2, … , \omega_d, b]^T \\X \in R^{d+1} &amp;= [x_1, x_2, … , x_d, 1]^T\end{aligned}$$ Error function &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在linear model当中，我们的目标变成了优化一个objective function，使得这个损失函数取到最小值，在linear model当中，objective function常见有两种，当然也是大同小异的： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1)Mean sum of square error: $$MSE(a) = \frac{1}{n}\sum_{i=1}^n(y_i - f(x_i, a))^2$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (2)Redual sum of square error: $$RSS(a) = \sum_{i=1}^n(y_i-f(x_i, a))^2$$ Optimize &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在我们得到了linear model的objective function之后，我们的目标就是找到一组参数$a$，使得上述两个objective function能够在training data上面取最小值，当然可以看到RSS和MSE其实都是用的最小二乘法的思想，优化目标都是一样的。我们对数据进行矩阵化的表示，$X = [x_1, x_2, … , x_n], y=[y_1, y_2, … , y_n]^T$. $$\begin{aligned}J_n(a) &amp;= (y-X^Ta)^T(y-X^Ta) \\\nabla_aJ_n(a) &amp;= -2X(y-X^Ta) = 0 \\a &amp;= (XX^T)^{-1}Xy\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 哦豁，到这里好像事情变得很简单，只需要对RSS求个导就能得到最优的参数解。当然，事情是没有想象的那么简单的，我们看$a = (XX^T)^{-1}Xy$, 如果$XX^T$不满秩的情况下，这个方程还能用吗，那当然是不能用了，因为$XX^T$都不能求逆了，怎么用！所以这个方程在$XX^T$满秩时有解，不满秩的时候存在无穷多解，也即overfitting problem. Statistic model of regression &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在这个part将会通过统计回归模型来推导linear model 的 objective function. 使用统计回归模型的时候，我们首先会假设存在一个隐含变量$\epsilon$, 所以我们linear model的表示形式就变成了： $$\begin{aligned}y &amp;= f(x, a) + \epsilon &amp;\epsilon \sim N(0, \sigma^2)\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 然后我们使用MLE的方法来最大化log-似然函数，所以整个推导过程如下： $$\begin{aligned}l(D, a, \sigma) &amp;= ln(l(D, a, \sigma)) \\&amp;= \sum_{i=1}^n lnp(y_i|x_i, a, \sigma) \\&amp;= \sum_{i=1}^n ln[\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(y-f(x, a))^2}] \\&amp;= -\frac{1}{2\sigma^2}\sum_{i=1}^n(y-f(x, a))^2 + c(\sigma)\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OK, Fine.这里我们看到当极大化似然函数的时候，我们得到和MSE或者是RSS的优化函数是一样的，所以这两种优化目标是相同的。 Over-fitting &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Overfitting problem 是机器学习问题当中很常见的问题，当选取的参数过多，而没有足够多的样本来支持的话，所以就会出现模型过拟合的问题。具体可以用以下这张图展示： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在之前的$a = (XX^T)^{-1}Xy$ 表达式当中，如果$XX^T$不满秩的话，最后结果导致就是overfitting problem. Ridge Regression &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 出了问题那就得解决是吧，出现了overfitting那么我们就得想办法解决这个问题。也就引出了接下来要阐述的正则化的解决方法。 $$a^* = argmin(\sum_{i=1}^n(y_i-x_i^Ta)^2 + \lambda\sum_{j=1}^pa_j^2)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 同样的添加正则项，同样也相当于给之前的objective function加上限制，也即： $$\begin{aligned}a^* = argmin(\sum_{i=1}^n(y_i-x_i^Ta)^2) &amp;&amp; s.t.\lambda\sum_{j=1}^pa_j^2\le t\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 最后对加上正则项的objective function进行求导然后就能求得极值。其实加上正则项的意义就在于，对于参数$a$进行惩罚，其中的参数$\lambda$就是对这两项进行相对应的调整，来决定这两项的比重。 $$a^* = (XX^T+\lambda I)^{-1}Xy$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 我们可以看一下参数$\lambda$对整个model的影响，当$\lambda$很大的时候，相当于正则加的很大，使得参数都很小，会使得$bias$变大。如果$\lambda$很小的时候，相当于是$variance$相对大一点。但是因为我们加上正则项，所以training error是不可避免的会增加，所以我们需要寻找的就是training error和testing error都相对偏小的那个平衡值： LASSO &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在上面我们提到正则项加的是$\lambda\sum_{j=1}^pa_j^2$, 这是很常用的L2 norm, 如果我们用一阶来做正则的话，就有一个新的名字，叫做LASSO,LASSO是一个Sparse model，也即自带featrue select的功能，LASSO的表达形式为： $$a^*=argmin(\sum_{i=1}^n(y_i-x_i^Ta)^2+\lambda\sum_{j=1}^p|a_j|)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 为什么说LASSO是一个Sparse model呢，我们从objective function的图形化表示来看就很容易理解，因为两条线的相切的地方总是在角点。所以很多项的参数就自动变成了0: Bias and variance decomposition &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 最后这里对bias和variance做一个讨论。bias主要指的是最后学到的model和真实的model之间的偏差，variance指的是每次学到的model之间误差值，noise是不可避免的误差，所以不能做到完全optimal的model.我们使用EPE进一步的衡量model loss： $$\begin{aligned}EPE(f) &amp;= \iint(y-f(x))^2p(x, y)dxdy \implies \\EPE(f) &amp;= (bias)^2+variance+noise \\(bias)^2 &amp;= \int[E_D(f(x;D)-E(y|x))]^2p(x)dx \\variance &amp;= \int E_D[f(x;D)-E_D(f(x;D))]^2p(x)dx \\noise &amp;= \int var(y|x)p(x)dx\end{aligned}$$]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beauty-of-Mathematics]]></title>
    <url>%2F2019%2F07%2F01%2FBeauty-of-Mathematics%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;最近很有幸能够拜读吴军博士的《数学之美》这本书，在这里记录一些自己在阅读当中的感受和体会，顺便也做一点摘抄啥的，日后能够回味当时看这本书的感受啦！ 1. 文字和语言vs数字和信息 通信模式：信息源 -&gt; (怪叫声，编码) -&gt; 信息（信道传输）-&gt;（听到的声音，解码）-&gt; 信息(接收者)，古人通过声音来相互传递信息，和现在的信息传输的模式无差。 罗塞塔石碑：罗塞塔石碑上记录了从古埃及至今的历史，破译过程归功于在石碑之上有三种不同语言版本的信息。所以作者通过此得到两个有用的结论： 信息冗余是信息安全的保障。只要有一份内容被完整的保留下来，那么信息就不会丢失，这对于现代的信道编码有一定的指导意义。 语言的数据，又被成为语料。双语或者是多语的对照语料，对于machine translation领域的研究是一个基础性的工程。 接下来进入到数字的部分，在最开始的时候，数字只是单纯的用来进行计数，而并没有更加高层的抽象概念表达。例如中国🇨🇳人对于不同位数数字的编码是个十百千万，这也就是最最朴素的编码方式，而解码方式就是我们很熟悉的乘法，例如20000 = 2x10x1000. 从象形文字到拼音文字的转变是一个飞跃，说明人类在描述物体的方式上加入了抽象的层次。同时我们也能在日常的文字当中找到信息论中最短编码原理的影子。比如说常用的文字一般笔画较少，而生僻字大多笔画比较多。这种文字设计方法本质上就是一种编码方式。 古犹太人抄写 《圣经》 的小故事：在抄写的时候难免会存在抄写错误的情况，那么聪明的古犹太人发明了一种和现代通信当中校验位思想很类似的一种方法。把每一个希伯来字母对应一个数字，然后每行文字加起来就能得到一个特殊的数字，同时对于每一列的文字进行同样的操作。这样对于每一行每一列都有一个唯一的校验位，当抄写完一页的时候，会去和原文逐一对比，如果一致就说明抄写无误了。 这章当中还提到了一个语言学研究方法的问题，到底是语言对，还是语法对.前者坚持从真实的语句文本，从语料出发，后者则是坚持从语言的规则出发入手，当然现在的NLP领域的成就宣告是前者的获胜。 2. 自然语言处理 – 从规则到统计 人类的语言本质上是一种编解码的过程，人想要将自己的所想表达出来，通过语言进行编码，然后其他人需要理解你的意思的话，需要对语言进行解码，从而获取到想要表达的意思。 最早提出机器智能思想的是阿兰图灵在1950年的一篇paper当中提出的，也即非常著名的图灵测试。 当前的NLP领域的成就全都靠的数学，更准确的说是靠的统计的支持。 NLP研究从最早的基于语法规则的方法转变到了基于数学统计的方法。同时NLP研究也从单纯的句法分析和语义理解，变成了非常贴近实际应用的MT、语音识别、数据挖掘、知识获取等等实际领域。 3. 统计语言模型 自然语言逐渐演变为上下文相关的信息表达和传递方式，一个基本问题就是为这种上下文相关的特性建立统计数学模型。也就是NLP当中非常重要的统计语言模型 马尔可夫假设：假设每个词出现的概率$\omega_i$只与前一个词$\omega_{i-1}$是相关的，也就是最后一个句子出现的概率表示为$p(S) = p(\omega_1) * p(\omega_2|\omega_1) * p(\omega_3|\omega_2) * … * p(\omega_n|\omega_{n-1})$. 这种模型又叫做二元模型，如果是之前的以前n个模型出现的概率来估计下一个词出现的概率，则这种模型叫做N元模型. 转换成这种形式之后，就能很方便的通过语料来解决统计的问题，根据大数定理，最后的频率估计就基本近似于实际的概率估计，也即：$$p(\omega_n|\omega_{n-1}) = \frac{p(\omega_n, \omega_{n-1})}{p(\omega_{n-1})} \tag{1}$$ 4. 分词]]></content>
      <categories>
        <category>Reading</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine-learning-1-DM-and-Bayesian]]></title>
    <url>%2F2019%2F06%2F29%2Fmachine-learning-1-DM-and-Bayesian%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;在这个系列的博客文章里面，主要记录的是本学期《数据挖掘导论》课程当中的知识点，涉及的是传统机器学习当中的算法，包括有监督学习、无监督学习这两大部分。这篇博客文章主要记录的是machine learning简介以及Bayesian的相关内容。 1. What is machine learningTwo kinds of problems 监督学习(supervised learning) 无监督学习(unsupervised learning) Differences: Supervised learning is trying to find a mapping from x to y, 比如说是classfication或者是regression的问题。 unsupervised learning问题是只有无label的样本数据，我们需要寻找interesting patterns, 比如说是聚类(clusters)、降维、矩阵分解(latent factors)等内容。 Supervised learning: collecting samples -&gt; define featrues -&gt; design and build models -&gt; make prediction Good representation for featrues: Low intra-class variability Low inter-class similarity Bias and variance trade-off(偏差和方差之间的权衡，主要是下面这张图体现)： Genelization(模型的泛化能力)：The performance on the testing data. 简单的模型的Train error更大，相对而言越复杂的模型的Train error会相对更小，但是Test error就不确定了，主要要参考bias和variance这两个基准来进行考量。 2. Bayesian Bayes’ Theorem 贝叶斯定理（这里我们只需要掌握贝叶斯公式即可）$$P(A|B) = \frac{P(B|A) * P(A)}{P(B)}$$ Prior: 先验知识，也即在进行分类之前我们能够得知的已知的样本信息，如果只是简单的根据先验知识来进行分类的话，只需要判断$\omega_1$ if $p(\omega_1) &gt; p(\omega_2)$ else $\omega_2$. likelihood: 似然，这里主要引进的是最大似然估计&nbsp;&nbsp;&nbsp; Assign input pattern $x$ to class $\omega_1$ if $p(x|\omega_1) &gt; p(x|\omega_2)$ else $\omega_2$. Posterior: 后验概率，在贝叶斯模型当中非常重要的一个点. $$\begin{aligned}p(\omega_i|x) &amp;= \frac{p(x|\omega_i) * p(\omega_i)}{p(x)} \\p(x) &amp;= \sum_{i=1}^c p(x|\omega_i) * p(\omega_i) \\ 1 &amp;= \sum_{i=1}^cp(\omega_i|x)\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Posterior = prior * likelihood / evidence, 不过因为使用贝叶斯进行后验估计的时候，evidence使用是相同的，所以不需要考虑这一项，我们只需要聚焦在先验概率和似然的计算上即可。 Optimal Bayes Decision rules &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在这里使用的就是后验概率来进行决策：if $p(\omega_1|x) &gt; p(\omega_2|x)$, decides $\omega_1$, otherwise $\omega_2$. 在此基础上，我们还可以证明，这种决策方式是错误概率最小的一种决策方式，这是因为： $$p(error|x) = min(p(\omega_1|x), p(\omega_2|x))$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Special cases:总共是两种特殊情况，当然也是从后验概率计算公式的角度出发进行考虑。 if $p(\omega_1) = p(\omega_2)$, then decide by if $p(x|\omega_1) &gt; p(x|\omega_2)$ then $\omega_1$ otherwise $\omega_2$ (Actually is Maximum likelyhood decision). if $p(x|\omega_1) = p(x|\omega_2)$, then decide by if $p(\omega_1) &gt; p(\omega_2)$ then $\omega_1$ otherwise $\omega_2$ (Actually is Prior). Bayes risks &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在Bayes risks当中引入了action和risk的概念，其中$\lambda(\alpha_i, \omega_j)$表示的是在 $\omega_j$ 类别之上 take $action_i$ 的 $risk$, 所以我们又可以得到一个新的优化目标，就是选取action当中的Bayes risks当中最小的。$$\begin{aligned}R(\alpha_i|x) &amp;= \sum_{j=1}^c\lambda(\alpha_i, \omega_j)p(\omega_j|x) \\R(\alpha_i) &amp;= \sum_{over x}R(\alpha_i|x)\end{aligned}$$ Discriminant function &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 由于在贝叶斯决策当中，我们选取的是后验概率作为我们的决策策略。所以可以得到贝叶斯的Discriminant function: $$g(x) = lnp(\omega_i|x) = lnp(x|\omega_i) + lnp(\omega_i)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 可以看到这个式子当中，先验从数据当中很容易获取，主要是似然的估计，所以我们需要对似然函数的参数进行估计。最直接而且最朴素的就是假设，数据在类别确定的时候是满足高斯分布或者正态分布，这样我们就能写出似然函数的形式（其中$\Sigma$为协方差矩阵, $\mu$为均值, $| |$为行列式值）： $$p(x|\omega_i) = p(x|\theta) = \frac{1}{(2\pi)^{\frac{d}{2}}|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$$ MLE and Bayesian Estimation &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 接下来一个很自然的问题就是，既然窝已经知道了似然函数的形式，那我的参数应该怎么去估计呢？别急，这里给出了两种用来估计最佳参数的方法，一种是Maximum Likelyhood Estimation, 也就是我们常说的极大似然估计，还有一种Bayesian Estimation这里就不太涉及，主要思想是提前假设参数本身也是符合一定的分布的，而在MLE当中参数是有一个最优的值，需要通过数据去估计这个最优的值。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 接下来我们来看看MLE是如何估计参数，顾名思义，需要去最大化数据集的似然。这里我们定义$p(D|\omega_i)$为数据集的似然函数，并且假设数据$x_1, x_2, … i.i.d$是独立同分布的数据，那么我们的数据集似然就能表示为所有数据的似然连乘，然后取一个对数似然变为连加，所以最后的log-likelyhood可以表示为如下的形式，然后我们只需要maximize这个似然函数即可： $$l(\theta) = lnp(D|\theta) = \sum_{i=1}^nlnp(x_i|\theta)$$ $$\theta^* = argmaxl(\theta) \implies \nabla_{\theta}l(\theta) = 0$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 最后分别对$\mu$和$\Sigma$进行求导，然后取最大值即可，详细的推导过程可以参考我在DM-assignment1当中的推导过程，采用的就是对矩阵进行求导，然后取极值的方式。So far, the MLE of all parameters for class k is shown below: $$\begin{cases}\hat{\mu_k} = \frac{1}{m} \sum_{i=1}^mx_i \\\hat{\Sigma_k} = \frac{1}{m}\sum_{i=1}^m(x_i-\hat{\mu_k})(x_i-\hat{\mu_k})^T \\\hat{\phi_k} = \frac{N_k}{N}\end{cases}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 不过在这里有一点是需要注意的是，我们可以看到协方差矩阵还可以写成这种形式，$\Sigma = \frac{1}{k}XX^T$, 所以当featrues数大于samples数的时候，矩阵$\Sigma$是不满秩矩阵，也就意味着这个时候是不能通过MLE来估计参数的，因为$|\Sigma| = 0$, 这个时候log-似然♾。 Back to discriminat function &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在之前我们讨论了如何利用MLE来估计似然函数的参数，在这里我们主要讨论几种discriminat function的形式，也就是$g(x)$函数的线性关系。我们首先将$p(x|\omega_i)$的高斯形式带入上面的log似然函数，得到： $$g(x) = -\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)-\frac{d}{2}ln2\pi-\frac{1}{2}ln|\Sigma|+lnp(\omega_i)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1)Case $\Sigma_i = \sigma^2I$ $$g(x)=-\frac{1}{2\sigma^2}(X^TX-2\mu_i^TX+\mu_i^T\mu_i) + lnp(\omega_i) = \omega_i^TX+\omega_{i0} \implies linear$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hyperplanes: $$\begin{aligned}g_i(x) = g_j(x)\end{aligned}$$ $$0 = (\frac{\mu_i-\mu_j}{\sigma^2})^Tx - \frac{\mu_i^T\mu_i - \mu_j^T\mu_j}{2\sigma^2} + ln\frac{p(\omega_i)}{p(\omega_j)}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (2)Case $\Sigma_i = \Sigma$, 即所有类的cov矩阵是相同的 $$g_i(x) = \mu_i^T\Sigma^{-1}x-\frac{1}{2}\mu_i^T\Sigma^{-1}\mu_i + lnp(\omega_i) \implies linear$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (3)Case $\Sigma_i = arbitary$ $$g_i(x) = x^T\omega_ix+\omega_i^Tx + \omega_{i0} \implies Quadratic$$ Error Probabilities &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这一个part会展示为什么选用后验是一个最优的策略，首先我们看两分类的问题： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1)two-classes $$\begin{aligned}p(error) &amp;= p(x\in R_2, \omega_1) + p(x\in R_1, \omega_2) \\&amp;= p(x\in R_2|\omega_1)p(\omega_1) + p(x\in R_1|\omega_2)p(\omega_2) \\&amp;= \int_{x\in R_2}p(x|\omega_1)p(\omega_1) + \int_{x\in R_1}p(x|\omega_2)p(\omega_2)\end{aligned}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (2)multi-classes $$p(correct) = \sum_{i=1}^c \int_{x\in R_i}p(x|\omega_i)p(\omega_i)$$ Naive Bayes classifier &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 经过前面的一系列铺垫之后，我们引入今天的最后一个主题，朴素贝叶斯分类器。思想非常朴素，就是利用的后验来进行决策。而后验正比似然乘上先验，先验知识很容易从数据当中得到，似然则假设的是不同featrues之间是不相关的，也即： $$p(\omega_i|x) \propto p(x|\omega_i)p(\omega_i) = p(x_1|\omega_i)p(x_2|\omega_i)…p(x_n|\omega_i)p(\omega_i)$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 到这里之后，我们只需要对每一个featrue进行一个似然的估计就可以了，根据数据的形式分为两种情况： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1)离散值： 在$\omega_i$类别当中直接数数量除一下就可以得到似然的值。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2)连续值： 这种情况之下假设$\omega_i$的数据符合高斯分布，分别去计算mean $\mu$ 和方差 $\sigma^2$, 然后用学到的高斯模型对输入的数据进行似然计算即可。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 最后要提一点就是拉普拉斯平滑，为什么要做平滑呢，因为在离散值的时候，很可能输入的数据在数据集当中是不存在，那么这个时候根据先前的计算公式计算得出该项为0，使得整个连乘的后验概率均为0，显然这是不合理的一种计算方法，所以我们需要进行平滑处理，具体的平滑操作是, 其中的K为$\omega_k$类当中总的类别数目： $$p(x_i|\omega_k) = \frac{|x_{ik}|+1}{N_{\omega_k}+K}$$ 3. Summary for Naive Bayes Robust to isolated noise points Handle missing values by ignoring the instance during probability estimate calculations Robust to irrelevant attributes Independence assumption may not hold for some attributes]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-basic]]></title>
    <url>%2F2019%2F06%2F23%2Fpython-basic%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;最近马上要python考试了，anyway虽然是基础课，还是稍微复习一下，在这里记录一下可能会有的坑和平时作业里面的一些错题，以后依一些其他基本的需要注意的东西都可以写到里面来～ Slideschapter1 Unicode码：为每个字符设定了统一的并且是唯一的二进制编码，用来满足跨平台的需求。 \u表示Unicode码。 utf-8编码：每一个ASCII码都有一个对应的utf-8编码，正好与8位的ASCII编码是一样的。py程序默认是utf-8编码格式，并且是不定长的编码方式。 py是一种解释型的语言。 标识符：由字符，下划线，数字组成，但是注意的是不能以数字开头. my_test _123 chapter2 二进制，八进制，十六进制：0b(B)、0o(O)、0x(X) 浮点数 运算(注意是运算) 存在误差：2.1-2.0==0.1 False. 并且浮点数的//运算输出也是浮点数。 科学计数法：12345 = 1.23e4, 0.012 = 1.2e-2. 复数：real+imag * j，其中real是实数部分，imag是虚数部分。 函数和方法：方法是依赖数据类型的另一种函数，一般方法是在类当中定义的叫做 类方法 。 多行字符串，是用&#39;&#39;&#39;xxx&#39;&#39;&#39;来表示多行字符串。在这种情况下给每一行的字符串自动加上一个\n来进行换行。 python支持多条件比较，也即1&lt;3&lt;5 == 1&lt;3 and 3&lt;5, 1&lt;3&gt;2 == 1&lt;3 and 3&gt;2, 所以这两个都是True。 一定要注意and 和 or 判断这两个坑，一个是and如果前面的已经False，后面就不会判断，or前面如果已经True，后面的也不会判断了。 3 and 5, 最后一个表达式的值作为整个表达式的值。 运算的优先级和方向：除掉**的方向是从右往左的，其他所有运算的方向都是从左往右的。 列表运算：加法[1, 2, 3] + [c, py], 乘法[1] * 3 -&gt; [1, 1, 1] int函数用法：int()-&gt;0, int(&quot;02&quot;), int(&quot; 35 &quot;), int(&quot;35&quot;, 8)表示的是八进制下的&quot;35&quot;代表的10进制数. 其他内置函数：bin(), hex(), oct(), ord(str)-&gt;unicode, chr(unicode)-&gt;str 序列不等的赋值：i, *j = &quot;123&quot;, j -&gt; [&#39;2&#39;, &#39;3&#39;] 格式化：^, &lt;, &gt;分别代表的是居中、左、右对齐方式。科学计数法：.2e保留两位小数的科学计数法。{index_1:x.yf}{index_2:x.yf}格式化字符串。format(value, &quot;x.yf&quot;) chapter3 列表逆序：lst[::-1]。 列表的浅复制：b = a[:], 这样复制出来的就是两个不同的列表。或者采用列表当中的方法：b = a.copy(). 原始字符串：r&#39;hello\n&#39; -&gt; hello\n, 字符串不可修改, find(str, start_pos, end_pos) title()首字母大写，lower(), upper() 字符串的format函数：{index:&lt;填充&gt;|^, &gt;, &lt;| &lt;宽度.精度&gt;&lt;格式&gt;}, 同时&#39;{:,}&#39;.format表示的是千分位数用逗号隔开的形式 列表方法：l.extend(x), l.pop(index)默认最后一个且会返回, l.remove(value), l.reverse(), l.sort(), l.insert(index, value)若index超则最后一个, 如果是index为负数的时候超则都是插入在第一个. a = (3) -&gt; int, a = (3,) -&gt; tuple, 并且元组只有count(x) 和 index(x)这两个函数 需要注意split函数： 123456sen = 'hello wor ld 's = sen.split()print(len(s)) -&gt; 3s = sen.split(' ')print(len(s)) -&gt; 10 随机函数库：random库 random.random() -&gt; [0.0, 1.0) random.uniform(a, b) -&gt; [a, b] random.randint(a, b) -&gt; 随机的一个整数 chapter4 Python map函数: map(function, iterable) 对指定的序列执行map操作，也即每个元素都执行function, x, y = map(int, input().split())执行的是内置函数int, 或者也可以是匿名函数map(lambda x : x**2, [1, 2, 3]). try-except使用格式： 1234567891011try: blockexcept Exception 1: block1except Exception 2: block2except: block3else: block4finally: block5 while使用格式： 1234while(condition): block1else: block2 Gcd算法 12345def gcd(a, b): if b == 0: return a else: return gcd(b, a % b) chapter5 set是一种容器，没有先后顺序，并且元素的值不重复, 需要注意的是集合的元素需要是不可变对象, 换句话说就是列表不能成为集合当中的元素. 空集合： 12empty = set()empty = &#123;&#125; #创建的是空字典 方法：add, remove, len, max, min, sum, issubset, issuperset, 需要牢记的是集合是无序的，所以每次print出来的结果是不确定的。 在集合当中的 == 判断的是两个set当中的元素是否是相同的，如果是相同的则输出的是True. &lt;, &lt;=, &gt;, &gt;=判断是真子集，子集，真超集，超集这些的。 集合运算|, &amp;, -, ^分别是并集、交集、差集、对称差，对称差指的是除了共同元素之外的元素，也就是a ^ b = (a | b) - (a &amp; b) 列表去重并且顺序不变两种方法： 123456789a = [1, 2, 6, 1, 3, 4, 2, 5]addr_to = list(set(a))addr_to.sort(key=a.index)a = [1, 2, 6, 1, 3, 4, 2, 5]addr_to = []for i in a: if not i in addr_to: addr_to.append(i) 字典的创建方式： 123fac = dict([(key1, value1), (key2, value2)])fac = dict(key1=value1, key2=value2) #这种方式需要使用标识符，并且键用的是str类型# -&gt; &#123;'key1':value1, 'key2':value2&#125; 字典的键必须要是不可变对象，可变对象比如列表，字典是不能作为字典的键的。 字典的方法：in, not in判断的是一个值是否存在字典的键当中。get(key, 默认值)这个函数可以在后面设置一个默认值作为不存在时候的返回，不加是None. chapter6 lambda表达式：g = lambda x, y, z : x + y + z -&gt; g(1, 2, 3) 位置参数: 就是普通的那种参数传递方式 关键字参数(可以允许位置不同时进行赋值)： 12345def dis(x1,y1,x2,y2): #求平面上两点距离 print("x1=&#123;&#125;,y1=&#123;&#125;,x2=&#123;&#125;,y2=&#123;&#125;".format(x1,y1,x2,y2)) return sqrt((x1-x2)**2+(y1-y2)**2)print(dis(x1=1,y2=5,y1=3,x2=4)) 位置参数与关键字参数一起使用：需要注意⚠️的是这种情况之下，位置参数需要先写，然后再使用关键字参数，否则会出错。 12345678910111213#correct usefrom math import sqrtdef dis(x1,y1,x2,y2): #求平 面上两点距离 print("x1=&#123;&#125;,y1=&#123;&#125;,x2=&#123;&#125;,y2=&#123;&#125;".format(x1,y1,x2,y2)) return sqrt((x1-x2)**2+(y1-y2)**2)print(dis(1,3,y2=5,x2=4))#wrong casefrom math import sqrtdef dis(x1,y1,x2,y2): #求平面上两点距离 print("x1=&#123;&#125;,y1=&#123;&#125;,x2=&#123;&#125;,y2=&#123;&#125;".format(x1,y1,x2,y2)) return sqrt((x1-x2)**2+(y1-y2)**2)print(dis(1,y1=3,4,5)) 默认值参数：这里就是需要注意的是，默认值参数在函数对象被创建的时候同时被创建，相当于如果默认值有一个list的时候，有一个静态的static的list一直存在： 12345def init(arg, result=[]): result.append(arg)print(result)init('a') #['a']init('b') #['a', 'b'] 不定长数目参数：使用*将可变的参数变为元组存放，具体的使用🌰： 12345def countnum(a,*b): #计算参数个数 print(b) #(7, 9) print(len(b)+1) # 3countnum(3,7,9)countnum(5,8,1,6,89) *或者是**作用在函数的形参上代表的分别是用元组和字典来接收参数。*或者是**作用在实参上代表的是解包： 1234567lst = [1, 2, 3]print(*lst) # 1 2 3def countnum(a,**d): #计算参数个数 print(d) print(len(d)+1)countnum(3,x1=9,x2=1,x3=6,x4=89) return如果没有则默认值是None 命名空间：全局变量和局部变量不一样，在局部区域里面可以使用全局变量，但是这个时候不能在局部函数内部使用了全局变量再定义同名变量，否则出错❌，会变成变量还未定义就使用。但是一旦定义了同名的变量，使用都是用的局部变量来使用。要使用和修改全局变量，请使用global关键字来完成。 内置函数zip用来打包📦两个可迭代的序列 123456789101112131415161718192021222324252627a = [1, 2, 3]b = [4, 5, 6]print(list(zip(a, b)))# 字典的键值对互换dic = &#123;1:1, 2:2, '1':500&#125;print(dict(zip(dic.values(), dic.keys())))# &#123;1: 1, 2: 2, 500: '1'&#125;# eval 计算表达式的值a = 3; b = 7print(eval('a * 3 + b * 5'))# exec 执行python语句exec('print("hello world")')'''all, any 接受的是可迭代的序列all表示全部是True，才返回Trueany表示只要有一个True，即结果最后返回的是True'''all([[], False, 0]) # Falseall([1, True, [1]]) # Trueany([[], False, 0]) # False# __main__的作用域就是全局域 命令行参数格式：sys.argv[0]文件名，sys.argv[1]…都是参数。 python包：是一个目录，当中要包含init.py，然后就是那种传统的包，可以进行import package.module chapter7 文件打开：open(filename, mode) 读写函数： read: 将文件所有内容读取 readlines: 按行读取内容，但是列表当中的每一项最后都有一个\n, readline也是一样的会有一个\n在读取的字符串最后。 多条件排序： 12ans=sorted(counts.items(), key=lambda x:(-x[1], x[0]))# 按照value降序，然后再按照key升序排列 chapter8 一切内容皆是对象，函数和类也是对象。 类主要指的是类型，对象是比较具体的一些值。lst = [1, 2, 3], 其中list是对象lst的class，也就是list类型的对象 类有自己的名字空间，同时每个对象也有自己的名字空间。 构造方法：def __init__(self, …) 构造方法，当对象被构造的时候被自动调用，可以声明类所产生的对象属性，并可为其赋初始值，最重要的是这个函数是不能有返回值的。-&gt; 创建对象的时候调用的是object本身的__init__方法。 self参数：必须为第一个形参，代表的是将来要创建的对象本身。在类的方法当中访问数据成员的时候，需要使用self.parameter进行调用。在外部通过对象调用实例方法的时候不需要传递self参数，如果在外部通过类来调用实例方法的时候，可以显式的把对象传到self里面。 python社团规定： _xxx 内部名，不应该在外部使用 __xxx py解释器会自动换名为_class__xxx，需要注意的是成员变量和类变量都会自动换名，所以对象是直接访问不到这个变量的.但是需要注意的是，可以使用object._class__xxx访问到该变量，所以并不是绝对意义上的私有变量。 __xxx__系统定义的特殊成员，一般不使用这种标识符。 类变量和实例变量都是可以动态增加对应的变量的. 继承的类如果重写了__init__, 如果要初始化父类需要显式的super().__init__调用父类的初始化函数。 python多态，一般是通过重写__add__, __len__, __sub__之类的函数来自定义自己的基本运算。 题目解析 python当中浮点数是float类型，并没有double型类型。 round(x, 2)表示的是保留两位小数的四舍五入法。round(18.67, 1) = 18.7, round(18.67, -1) = 20.0, round(18.67, 0) = 19.0, floor(), ceil(), int(&quot;20&quot;, 16) = 32. 逻辑值not, or, and如果有多个则是一步一步的判断过去的。从左到右的direction, 就是说or或者and左右两边都要是一个表达式. math.sin(x)其中x是弧度制表示的，所以用角度值需要进行相对应的制度转换. 0 and anything的结果都是0，其他的value1 and value2结果表达式都是value2. list只有index方法，对于字符串而言是有index和find两种方法的。其中find没找到是返回-1, index没找到的话直接是报错。 list的pop(index)按照index，默认是pop列表的最后一个值。remove(value)是按照value来remove的. 浮点数的相等是看内部的浮点数表示，所以如果两个变量引用同一个常量浮点数的话，相等是True，因为本身就是同一个常量引用x = 2.0;y=2.0 (x==y) = True. 但是如果涉及到浮点数的运算的时候需要特别注意，一般来说表示方式就有变化，所以很大概率并不是一个值了。并且低精度在和高精度比较的时候，会自动转换到高精度. 12345x=2;y=2.0 #分号可把两个语句写在一行if(x == y): print("相等") # correct answerelse: print("不相等") 双重循环list的🌰： 123456l3=[[(i,j) for i in range(1,6)] for j in range(1,6)]print(l3[2][1][0])# 其中j负责外层的创建新列表，i负责内层和j一同生成元组值，# [[(1, 1), (2, 1)...], [(1, 2), (2, 2)...]]# 然后根据对应关系来找值即可 以只写模式打开的文件不能进行写操作，但是以写模式打开的文件有可能可以进行写操作。 在python当中变量不需事先声明就可使用。 注意判断条件： 12345678910if number%2 and not number%3:# 代表的是number不是2的倍数，但是是3的倍数b=[1,2,3]b[2]=b# 这种情况下是无限循环的情况if v &lt; values[row][column]: v = values[row][column]# 注意求的是最大值，而不是最小值 🐒经典题： 12345678910n = int(input().strip())ls = [i for i in range(1, n + 1)]index = -1while len(ls) &gt; 1: index = (index + 3) % len(ls) del ls[index] index -= 1 # 注意这里要把index-1print(ls[0]) 在三引号的字符串中可以包含单引号，双引号字符. 不过在单独的单双引号的时候，需要考虑转义或者单引号利用双引号这种不同的表示形式。 注意在多变量赋值的时候,x = y = z + 1是不会出错的，但是x = (y = z + 1)是出错的，因为python自动将y = z + 1作为一个表达式处理，但是又没有返回值，所以出错！！ list的sort方法是没有返回值的。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>basis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Old Books Trading Website]]></title>
    <url>%2F2019%2F06%2F22%2FWebsite%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;陆陆续续写了大概两周时间的旧书交易网站终于快写完啦！！BS课程的课程项目设计，真是心力交瘁。刚好新搞了一个博客试试水，就在这里记录一些开发过程当中的感想和碰到的很多很多坑吧orz，不过还是学到很多东西的吖！ 后端框架&nbsp;&nbsp;&nbsp;&nbsp;后端框架选用的是Django框架开发的，为什么选Python作为后端的语言呢，一方面是这学期刚好也有在上python课程，另一方面python真的很友好啊，不过以后也要尝试用java spring做点小项目。 Django路由&nbsp;&nbsp;&nbsp;&nbsp;Django框架遵循的是MVC的思想进行开发的框架。urls.py当中是网站路由的配置，一般项目的路由当中是划定不同app下的起始路由，然后在不同的app下再定义各自的url，使得整个项目的路由管理比较独立，比如在我项目当中的总路由配置(BookTradeWeb/urls.py): 123456789urlpatterns = [ path('admin/', admin.site.urls), url(r'^password-reset/', include('password_reset.urls')), url(r'^auth/', include('useraction.urls')), url(r'^books/', include('books.urls')), url(r'^chatting/', include('chatting.urls')), url(r'^media/(?P&lt;path&gt;.*)$', serve, &#123;'document_root': settings.MEDIA_ROOT&#125;), url(r'^$', TemplateView.as_view(template_name="login.html")),] &nbsp;&nbsp;&nbsp;&nbsp;然后在每个单独的app里面可以自定义自己的路由了，以我其中一个app(useraction)下的urls.py为🌰，不过需要记住的是通过python manage.py startapp 创建之后的urls.py是需要自己创建的。在不同的app里面可以定义自己的app_name，在后面写模板语言的时候非常爽的，后面在前端那里再详细写： 1234567app_name = 'useraction'urlpatterns = [ url(r'^login/$', views.UserLoginView.as_view(), name='login'), url(r'^register/$', views.UserRegisterView.as_view(), name='register'), url(r'^logout/$', views.UserLogoutView.as_view(), name='logout'),] Django Views&nbsp;&nbsp;&nbsp;&nbsp;Django views可以说是很强大了，在MVC架构当中它可以充当view和controller两部分的角色。现在很流行使用restful的方式开发网站。也即后端只提供数据，用json或者protobuf或者其他的数据组织形式来传输数据，而前端向后端发送请求来请求数据或者处理逻辑，实现前后端分离的开发。&nbsp;&nbsp;&nbsp;&nbsp;Django views因为有很多Django自己的加持，使得这两种模式都支持，比如JsonResponse返回json格式的数据，或者通过render(request, index.html, data)的方式直接返回网页，并且能用非常爽的模板语言，简直人见人爱啊有木有！&nbsp;&nbsp;&nbsp;&nbsp;这里我选择的是用的class-based view的方式来写views.py的，在BookTradeWeb/utils.py当中定义了BaseView，这个类继承了Django当中内置的view，后面我们自己的view只需要继承BaseView即可，我们看看BaseView里面定义了些啥： 123456789101112131415161718192021222324252627282930313233class BaseView(View): def dispatch(self, request, *args, **kwargs): self.request = request request.data = &#123;&#125; if request.method == 'GET': request.data = request.GET.dict() elif request.content_type == 'application/json': request.data = request.body.replace(b"'", b'"') request.data = json.loads(request.data) elif request.content_type == 'multipart/form-data': request.data = request.POST.dict() if not request.FILES == None: request.data['file'] = request.FILES['file'] elif request.content_type == 'application/x-www-form-urlencoded': request.data = request.POST.dict() return self.do_dispatch(request, *args, **kwargs) def do_dispatch(self, *args, **kwargs): handler = getattr(self, self.request.method.lower(), None) if not callable(handler): return JsonResponse(make_errors(f"http method &#123;self.request.method.lower()&#125; not allowed ")) else: try: data = handler(*args, **kwargs) if data == None: return JsonResponse(make_success('success')) elif self.request.method == 'GET' or not isinstance(data, dict): return data else: return JsonResponse(&#123;'data': data, 'code': 0&#125;) except Exception as e: traceback.print_exc() return JsonResponse(make_errors(str(e))) &nbsp;&nbsp;&nbsp;&nbsp;这样定义BaseView有什么好处呢，首先在BaseView里面重写了View当中的dispatch方法，每当有一个请求到达对应路由那里的时候，都会调用dispatch方法。在dispatch当中我们将数据统一都放到request.data里面，后面的views当中无论是POST还是GET都可以在request.data里面拿到数据。&nbsp;&nbsp;&nbsp;&nbsp;其次在do_dispatch当中会去自定义view里面找对应request method的处理方法，也就是说，我们的自定义view里面只需要去写对应的函数，比如说def get(self, request) 处理get请求，def post(self, request)来处理对应的post请求，是不是很方便呢！在自定义view写完之后，只需要在urls.py里面去注册一下就好了，下面举一个🌰： 12345678910111213141516class UserLoginView(BaseView): def post(self, request): user = authenticate(username = request.data.get('username'), password = request.data.get('password')) if user and user.is_active: login(request, user) return user.to_dict() elif not user: raise Exception('用户名或密码错误！') def get(self, request): return render(request, 'login.html')useraction/urls.pyurl(r'^login/$', views.UserLoginView.as_view(), name='login') Django models&nbsp;&nbsp;&nbsp;&nbsp;Django提供了一套非常完备的ORM，以至于我真的一行SQL语句都没写啊😂。models也就是数据库里面的表，通过类定义的方式来定义每一张表当中的数据项、属性、外键约束等等。一般定义好models之后只需要执行python manage.py makemigrations &amp;&amp; python manage.py migrate将models迁移到数据库当中。Django项目初始默认使用的数据库是sqlite，我这里将它改成了MySQL，具体就是修改setting.py里面的配置就好了。&nbsp;&nbsp;&nbsp;&nbsp;ORM的语法网上教程或者是Django官网教程都有很多的。主要就是内部用的一个QuerySet类来代表返回结果，还有很多奇奇怪怪的用法这里就不再赘述。 Django setting&nbsp;&nbsp;&nbsp;&nbsp;Django 通过在项目目录下的setting.py下面进行整个项目的配置，大概讲解一下这些配置是干啥的，就讲一些主要的，其他的都是很容易Google到的： ALLOWED_HOSTS: 允许那些host访问，如果需要允许所有一般设置为[‘*’, ] INSTALLED_APPS: 这个配置是比较重要的一部分，用来注册项目里面的app，如果你新建的app没有在这里注册的话，项目是找不到你的app里面的方法的，所以我们每次新建都需要在这里进行注册之后才能正常的工作哦😯 Middleware: 这里是项目中使用的中间件的配置，比如Django内置的用户认证中间价，防跨站攻击的csrftoken中间价等等，如果使用其他的开源中间价都需要在这里进行注册。 TEMPLATES: 这里主要是存放的静态文件的意思，基本不用去改，Django会去项目所有的templates文件夹下面找对应的模板文件。 DATABASES: 顾名思义就是数据库的配置了，你想用哪个数据改改这个就好了。 1234567STATIC_URL = '/static/' STATICFILES_DIRS = [ os.path.join(BASE_DIR, 'BookTradeWeb/static') ] MEDIA_URL = '/media/' MEDIA_ROOT = os.path.join(BASE_DIR, 'media/') 这两个是静态文件和资源文件的存放位置，Django会去static和media文件夹里面自动寻找。 前端模板&nbsp;&nbsp;&nbsp;&nbsp;前端还是主要用的传统的html+js+css的格式，然后通过ajax向后端发起请求，没能用现在很流行的React或者是Vue有点可惜，只能感慨前端太难了！！.不过Django自己的模板语言用起来还是可以的，不过我感觉这个项目调前端的时间是最主要的啊orz. 静态文件加载&nbsp;&nbsp;&nbsp;&nbsp;之前我们在setting里面配置好了static文件目录之后，在模板里面我们可以这样引入，在最开头通过 12&#123;% load staticfiles %&#125; 加载静态文件&#123;% static 'images/favicon.png' %&#125; 引入对应文件 &nbsp;&nbsp;&nbsp;&nbsp;注意Django会搜索所有的static文件夹下的文件，所以一般使用app名称作为前缀来进行区分。 前端路由&nbsp;&nbsp;&nbsp;&nbsp;在之前我们提到每个urls.py里面我们可以添加一个app_name字段，并且每个url_pattern里面都可以带上一个name字段，然后在前端就可以直接用模板语言当中的url &#39;app_name:name&#39;来获取后端定义好的路由，这样就算你后端改了路由之后，前端根本不用管啊是不是～ 123456useraction/urls.pyapp_name = 'useraction'url(r'^register/$', views.UserRegisterView.as_view(),name='register')template:&#123;% url 'useraction:register' %&#125; 前端数据渲染&nbsp;&nbsp;&nbsp;&nbsp;有两种方式，一种是通过ajax向后端请求，然后用js来进行数据渲染，还有一种是通过模板语言，在render方法的时候将数据传递到模板里面，然后使用模板语言渲染，常用的有： 12345678910111213&#123;&#123; data &#125;&#125; 取出传递进来数据当中的data字段&#123;% if ... %&#125;&#123;% else %&#125;&#123;% endif %&#125; 模板语言当中的if语句&#123;% for book in books %&#125;&#123;&#123; book.data &#125;&#125;&#123;% endfor %&#125; 模板语言当中的for语句&#123;&#123; book.time|data:"Y-M-d" &#125;&#125; 模板语言当中的filter过滤器&#123;&#123; request.user &#125;&#125; 模板当中获取当前请求的user &nbsp;&nbsp;&nbsp;&nbsp;还有很多其他的用法大家自己网上找找也能找到。这里主要将还可以自定义自己的filter，一个很直观的需求就是我像模板里面传进了一个字典，窝怎么根据键来找值呢，Django提供了自定义filter的功能，在app目录下面新建一个templatetags文件夹，首先新建一个__init__.py为空不要紧，但是一定要有。然后建一个自定义的filter文件，这里我是books_filter.py，通过@register.filter标注为一个自定义的filter，然后就写你想要的filter逻辑即可： 123456789101112from django.template.defaulttags import registerfrom BookTradeWeb.utils import Categoryfrom books.models import Book@register.filterdef GetDictValue(dictionary, key): assert isinstance(dictionary, dict) return dictionary.get(key)模板中使用：首先最上方引入你的filter &#123;% load books_filter %&#125;然后用法：&#123;&#123; book_dict|GetDictValue:key &#125;&#125; Websocket&nbsp;&nbsp;&nbsp;&nbsp;由于Django本身是不支持websocket的，但是这次要求要集成一个消息系统，所以想到的是websocket这个全双工的协议。Google了一下，可以通过django-channels第三方的package来实现✅，所以直接pip安装就好。channels是用的redis做消息缓存，所以还要pip install channels-redis，这些需要的依赖一般都是写在requirements.txt当中。如果不想在本机安装redis可以通过docker来开启服务，具体就是开一个docker container就好，docker run -d -p 6379:6379 redis:latest, 搞定。&nbsp;&nbsp;&nbsp;&nbsp;在setting里面注册channels这个app，然后添加如下的设置: 123456789ASGI_APPLICATION = "BookTradeWeb.routing.application"CHANNEL_LAYERS = &#123; 'default': &#123; 'BACKEND': 'channels_redis.core.RedisChannelLayer', 'CONFIG': &#123; "hosts": [(CONFIGS['REDIS_HOST'], CONFIGS['REDIS_PORT'])], &#125;, &#125;,&#125; &nbsp;&nbsp;&nbsp;&nbsp;asgi配置主要是用来处理websocket请求的，类似ws://这样的请求Django自己的服务器是处理不了的，所以这里用asgi配置，需要在app目录下面新建一个routing.py用来处理websocket路由。 1234567891011121314151617项目目录(BookTradeWeb/routing.py)application = ProtocolTypeRouter(&#123; # (http-&gt;django views is added by default) 'websocket' : AuthMiddlewareStack( URLRouter( chatting.routing.websocket_urlpatterns ) ),&#125;)app目录下(chatting/routing.py)from django.conf.urls import urlfrom . import consumerswebsocket_urlpatterns = [ url(r'^ws/chat/(?P&lt;room_name&gt;[^/]+)/$', consumers.ChatConsumer),] &nbsp;&nbsp;&nbsp;&nbsp;经过这样一通配置，开启python manage.py runserver，看到ASGI/Channels就配置成功了。 Consumer&nbsp;&nbsp;&nbsp;&nbsp;在channels当中处理websocket请求的东西叫做consumer，和上面我们提到的view一样，consumer也是可以自定义的，可以继承自AsyncWebsocketConsumer来实现自己的consumer。主要是几个functions重写一下就好了，connect, disconnect, receive，都是字面意思了就不解释了。每一个websocket连接都会给到一个channel给连接的用户。每一个channel都有一个独有的channel_name, 通过self.channel_name即可获取。 Group&nbsp;&nbsp;&nbsp;&nbsp;Group组的概念，聊天室必备啊。group以group name来进行区分，不同的channel都可以加入到group里面，group当中提供了group_send来进行广播📢，也就是聊天室的那种效果了。 channel_layer&nbsp;&nbsp;&nbsp;&nbsp;Django-channels当中提供了get_channel_layer()方法来向指定的channel发送消息，具体使用如下，也就是我们要的点对点聊天的功能了： 123456789101112131415161718from channels.generic.websocket import AsyncWebsocketConsumerfrom channels.layers import get_channel_layerfrom channels.db import database_sync_to_asyncfrom collections import defaultdictfrom useraction.models import Userfrom chatting.models import ChattingMessageimport jsonchannel_layer = get_channel_layer()await channel_layer.send( channel_name, &#123; "type" : "chat.message", "message" : message, "send_side" : send_side, "option" : "chat" &#125;) Database&nbsp;&nbsp;&nbsp;&nbsp;在channels当中，我们的处理方式都是异步的，比如函数定义都是async, 函数使用都是await, 那对数据库操作一般Django都是同步的操作，这样在channels里面运行是没有任何作用的，所以这个时候我们使用注解让数据库的操作变成异步的@database_sync_to_async, 然后对数据库的操作写在一个函数里面，使用🌰： 12345678910from channels.db import database_sync_to_async@database_sync_to_asyncdef CreateNewMessage(self, send_side, recv_side, message): mess = ChattingMessage.objects.create( send_side_id=int(send_side), recv_side_id=int(recv_side), message=message ) mess.save() 部署方法&nbsp;&nbsp;&nbsp;&nbsp;这个项目我使用的是docker进行部署，总共是3个container，其中包括mysql, redis, website这三个container，服务端口列表为: port service 6379 redis 3000 mysql 8000 website docker-compose&nbsp;&nbsp;&nbsp;&nbsp; 我用的是docker-compose来管理docker container之间的连接关系，重要的配置文件窝都写在项目目录下的config.json里面，docker-compose方便进行container之间的连接，并且自动建立不同container之间的network，能让多个container进行互相访问，非常的方便，用的时候是写docker-compose.yml： 1234567891011121314151617181920212223242526version: '3'services: db: image: mysql:latest command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8 --collation-server=utf8_general_ci ports: - "3000:3306" environment: MYSQL_DATABASE: 'booktrade' MYSQL_ROOT_PASSWORD: 'your-password' redis: image: redis:latest restart: always expose: - "6379" web: build: . command: ./scripts/start_server.sh entrypoint: ./scripts/docker_entry.sh db 3306 restart: always ports: - "8000:8000" depends_on: - db - redis &nbsp;&nbsp;&nbsp;&nbsp;需要注意的是，这里有一个大坑，就是web container在开启的时候，是和其他服务一起开启的，所以在mysql container还没初始化好的时候，web container就开始尝试写数据到数据库里面去了，当然，这样服务器of course crash了，所以在entrypoint这里加了一个脚本去检查，如果没有初始化好就sleep去等，初始化完毕之后再去连接数据库： 12345678910111213141516171819scripts/docker_entry.sh#!/bin/shpostgres_host=$1postgres_port=$2shift 2cmd="$@"# wait for the postgres docker to be runningwhile ! nc $postgres_host $postgres_port; do &gt;&amp;2 echo "MySQL is unavailable - sleeping" sleep 1done&gt;&amp;2 echo "MySQL is up - executing command"# run the commandexec $cmd &nbsp;&nbsp;&nbsp;&nbsp;web项目的Dockerfile主要就是初始化环境，安装一些package的依赖： 12345678910111213FROM python:3.6MAINTAINER huangyifei &lt;huangyifei0910@gmail.com&gt;RUN mkdir /codeADD . /codeWORKDIR /codeRUN wget -qO- git.io/superupdate.sh | bashRUN apt-get -y install jq netcat-openbsdRUN jq '.DB_HOST = "db"' config.json &gt;&gt; tmp.json &amp;&amp; mv tmp.json config.jsonRUN jq '.REDIS_HOST = "redis"' config.json &gt;&gt; tmp.json &amp;&amp; mv tmp.json config.jsonRUN pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt &nbsp;&nbsp;&nbsp;&nbsp;在所有这些做完之后，使用docker-compose build构建当前项目，docker-compose up -d进行container的链接和后台运行，因为mysql container的初始化时间比较长，所以开启之后需要等待20-30s然后就可以愉快的访问啦(localhost:8000)～ 窝是将这个步骤写在./scripts/start_docker.sh里面，./scripts/stop_docker是停止所有的服务，并且移除相对应的container，即插即用，是不是很方便！！ 项目总结&nbsp;&nbsp;&nbsp;&nbsp;第一次用Django做后端和docker-compose来部署项目还是踩了不少的坑orz，不过Django强大的特性和很多第三方的package加持，做后端的很多功能也是挺方便的。docker主要是这学期开始接触的，我觉得这东西势必会统治虚拟化啊，即插即用，生态也非常的好，用来部署项目省了很多乱七八糟的配置的问题。后面也会考虑去学一学前端的框架，React or Vue anyway, 还有很多需要学习的，任重而道远啊！Make Progress Everyday！ 项目地址：BookTradeWeb 使用方法：参考README.md当中的部署方法 部署好的网站：website Author: huangyifei Email: huangyifei0910@gmail.com]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>docker-compose</tag>
        <tag>websocket</tag>
        <tag>js</tag>
      </tags>
  </entry>
</search>
