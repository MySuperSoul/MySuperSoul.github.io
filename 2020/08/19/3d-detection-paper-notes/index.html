<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="7LSmUZy8Q4YDUNKKdehjSOm19BfZkmsgS9hDwrgVWOo" />
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>
<div class="top-scroll-bar"></div>







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/32x32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/16x16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="3d-detection," />










<meta name="description" content="This post will mainly contain the paper notes about the papers in 3d object detections field, including the lidar-based or image-based or multi-modal methods. This post will continuously updating.">
<meta property="og:type" content="article">
<meta property="og:title" content="3d-detection-paper-notes">
<meta property="og:url" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/index.html">
<meta property="og:site_name" content="Huangyifei&#39;s Blog">
<meta property="og:description" content="This post will mainly contain the paper notes about the papers in 3d object detections field, including the lidar-based or image-based or multi-modal methods. This post will continuously updating.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/1-1.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/1-2.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/1-3.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/1-4.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/2-1.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/2-2.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/3-1.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/3-2.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/3-3.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/4-1.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/4-2.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/4-3.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/4-4.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/5-1.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/5-2.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/5-3.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/5-4.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/6-1.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/6-2.png">
<meta property="og:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/6-3.png">
<meta property="article:published_time" content="2020-08-19T14:48:16.000Z">
<meta property="article:modified_time" content="2020-09-05T08:17:49.871Z">
<meta property="article:author" content="huangyifei">
<meta property="article:tag" content="3d-detection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/1-1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: false,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/"/>





  <title>3d-detection-paper-notes | Huangyifei's Blog</title>
  








<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/MySuperSoul" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Huangyifei's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Never forget to say "thanks"</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mysupersoul.github.io/2020/08/19/3d-detection-paper-notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="huangyifei">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars2.githubusercontent.com/u/28039767?s=400&u=43e653f3e1abe2fa56182133d26fc581ed4b2b40&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huangyifei's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">3d-detection-paper-notes</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-19T22:48:16+08:00">
                2020-08-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computer-vision/" itemprop="url" rel="index">
                    <span itemprop="name">computer-vision</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/08/19/3d-detection-paper-notes/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2020/08/19/3d-detection-paper-notes/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2020/08/19/3d-detection-paper-notes/" class="leancloud_visitors" data-flag-title="3d-detection-paper-notes">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  7.2k words
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  29 minutes
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This post will mainly contain the paper notes about the papers in 3d object detections field, including the lidar-based or image-based or multi-modal methods. This post will continuously updating.<br><a id="more"></a></p>
<hr>
<h2 id="Multi-View-3D-Object-Detection-Network-for-Autonomous-Driving-CVPR-2017"><a href="#Multi-View-3D-Object-Detection-Network-for-Autonomous-Driving-CVPR-2017" class="headerlink" title="Multi-View 3D Object Detection Network for Autonomous Driving (CVPR 2017)"></a><a href="https://arxiv.org/abs/1611.07759" target="_blank" rel="noopener">Multi-View 3D Object Detection Network for Autonomous Driving (CVPR 2017)</a></h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>MV3D将Lidar的点云和相机的RGB图片同时作为输入来预测3d的物体框。与以往的基于voxel的方法不同，MV3D利用了点云的鸟瞰图(BEV)和前视图(front-view)，能够避免掉3d卷积的大计算量，也不会损失过多的信息。随后生成3d的regions, 通过region-fusion sub-network对多个view生成的features进行融合，输出最终的3d检测框。</p>
<h3 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h3><p><strong>基于点云的检测</strong></p>
<ul>
<li><strong>方法</strong>：将3d点云编码为体素网格，然后提取结构特征进行分类。还有些方法利用3D的卷积神经网络增强特征编码。还另外有一些工作将点云投影到2d前视图，然后从前视图预测3d的检测框。</li>
<li><strong>缺陷</strong>：计算量太高</li>
</ul>
<p><strong>基于图片的3D检测</strong></p>
<ul>
<li><strong>方法</strong>： 通过3D体素模式应用一系列的ACF detectors去做2d检测和3d的姿态估计。</li>
<li><strong>缺陷</strong>：依赖于准确的深度估计和特征点检测，同时相对而言精度会更低一些，从2d图片还原3d的检测框。</li>
</ul>
<p><strong>多模态融合</strong></p>
<ul>
<li><strong>方法</strong>：结合图像、光流、点云等进行检测</li>
<li><strong>缺陷</strong>：当时相关的研究还挺少，主要是如何融合多个模态的信息。</li>
</ul>
<h3 id="3-MV3D网络"><a href="#3-MV3D网络" class="headerlink" title="3. MV3D网络"></a>3. MV3D网络</h3><p>网络的整体结构如下图所示，主要包括两部分的子网络，一个是3D proposal network, 一个是region-based fusion network.</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/1-1.png" style="width: 95%">
</div>

<p>具体的步骤可以分为以下几个步骤：</p>
<ul>
<li>特征提取，包括Bird eye view、front view、RGB images</li>
<li>从俯视图当中生成3D proposals</li>
<li>将proposal投影到前视图和图片上，形成regions</li>
<li>每个regions通过roi-pooling生成固定长度的特征向量</li>
<li>三个特征向量输入fusion子网络进行特征融合</li>
<li>最后进行分类和3d检测框的回归</li>
</ul>
<p>下面具体对每个步骤进行相关的介绍：</p>
<h4 id="3D点云表示"><a href="#3D点云表示" class="headerlink" title="3D点云表示"></a>3D点云表示</h4><h5 id="Bird-eye-view"><a href="#Bird-eye-view" class="headerlink" title="Bird eye view"></a>Bird eye view</h5><p>点云的俯视图表示，主要由<code>高度</code>、<code>密度</code>、<code>强度(intensity)</code>三部分组成，投影到分辨率为<code>0.1米</code>的二维网格当中。</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/1-2.png" style="width: 95%">
</div>

<ul>
<li><strong>高度</strong></li>
</ul>
<p>为了更多的保留高度的信息，论文当中会对点云划分<code>M</code>个切片，然后在每个切片上，对每个grid当中的点云高度的最大值，作为该grid的高度值。最后会得到<code>M</code>个高度图。</p>
<ul>
<li><strong>密度</strong></li>
</ul>
<p>密度指代的是在每一个cell当中的点云的数量。对于每个cell当中的密度计算的公式：$\min \left(1.0, \frac{\log (N+1)}{\log (64)}\right)$</p>
<ul>
<li><strong>强度</strong></li>
</ul>
<p>强度的计算是以在每个cell当中高度最高的点的反射率作为该cell的强度值。</p>
<p>所以最后俯视图的特征图的channels总共会有<code>M + 2</code>层。其中M层是高度，另外两层分别是密度和强度的特征图。</p>
<h5 id="Front-View"><a href="#Front-View" class="headerlink" title="Front View"></a>Front View</h5><p>前视图作为俯视图的一个补充，因为点云往往都是很稀疏的，所以这里投影到前视图当中生成一个密集的2D point map。前视图的生成是通过将三维点云坐标投影到柱面上生成的二维图，投影公式如下：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
c=\lfloor\operatorname{atan} 2(y, x) / \Delta \theta]\rfloor \\
r=\left\lfloor\operatorname{atan} 2\left(z, \sqrt{x^{2}+y^{2}}\right) / \Delta \phi\right\rfloor
\end{array}</script><p>其中$p = (x, y, z)$为点云的3d坐标, $p_{fv} = (r, c)$为前视图坐标。</p>
<h5 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h5><p>提取图片特征就是直接用那些常用的骨干网络就好了。</p>
<h4 id="3D-proposal-network"><a href="#3D-proposal-network" class="headerlink" title="3D proposal network"></a>3D proposal network</h4><p>3D的候选区域是从俯视图当中生成的，使用俯视图有以下几个好处：</p>
<ul>
<li>当投影到BEV时，物体的大小关系被保留，也即在大小的方差变化很小</li>
<li>BEV当中的物体都占据着不同的space，不会有遮挡的问题。</li>
<li>在道路环境下，物体基本都位于水平地面上，所以在垂直方向方差比较小，对于生成准确的3d候选框比较关键。</li>
</ul>
<p>候选网络基本和RPN类似，候选的3d框用参数$(x, y, z, l, w, h)$进行表示，用4种类型的anchors来做RPN。这部分当中还有一些其他的tricks，包括反卷积扩大特征图尺寸，回归center offset、计算积分图去除empty anchor。</p>
<h4 id="Region-based-Fusion-network"><a href="#Region-based-Fusion-network" class="headerlink" title="Region-based Fusion network"></a>Region-based Fusion network</h4><p>上面我们说到了3d框额度参数为$(x, y, z, l, w, h)$表示，所以通过转换方程能够比较方便的得到在BEV、Front和图像平面上的region区域。而后通过ROI-pooling层从3个views中生成相同维度的feature vector。</p>
<script type="math/tex; mode=display">
f_{v}=\mathbf{R}\left(x, \mathrm{ROI}_{v}\right), v \in\{\mathrm{BV}, \mathrm{FV}, \mathrm{RGB}\}</script><h5 id="Deep-fusion"><a href="#Deep-fusion" class="headerlink" title="Deep fusion"></a>Deep fusion</h5><p>在Fusion过程当中，论文当中指出有三种fusion的方式，分别是:</p>
<ul>
<li><code>early fusion</code></li>
</ul>
<script type="math/tex; mode=display">
f_{L}=\mathbf{H}_{L}\left(\mathbf{H}_{L-1}\left(\cdots \mathbf{H}_{1}\left(f_{B V} \oplus f_{F V} \oplus f_{R G B}\right)\right)\right)</script><ul>
<li><code>late fusion</code></li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
f_{L}=&\left(\mathbf{H}_{L}^{B V}\left(\cdots \mathbf{H}_{1}^{B V}\left(f_{B V}\right)\right)\right) \oplus \\
&\left(\mathbf{H}_{L}^{F V}\left(\cdots \mathbf{H}_{1}^{F V}\left(f_{F V}\right)\right)\right) \oplus \\
&\left(\mathbf{H}_{L}^{R G B}\left(\cdots \mathbf{H}_{1}^{R G B}\left(f_{R G B}\right)\right)\right)
\end{aligned}</script><ul>
<li><code>deep fusion</code></li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
f_{0}=& f_{B V} \oplus f_{F V} \oplus f_{R G B} \\
f_{l}=& \mathbf{H}_{l}^{B V}\left(f_{l-1}\right) \oplus \mathbf{H}_{l}^{F V}\left(f_{l-1}\right) \oplus \mathbf{H}_{l}^{R G B}\left(f_{l-1}\right) \\
& \forall l=1, \cdots, L
\end{aligned}</script><div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/1-3.png" style="width: 95%">
</div>

<h5 id="Oriented-3d-box-regression"><a href="#Oriented-3d-box-regression" class="headerlink" title="Oriented 3d box regression"></a>Oriented 3d box regression</h5><p>相比于传统的中心点+长宽高+orientations的编码，这里直接回归八个角点相对于真实点的偏差，所以会有8*3=24个偏移量需要进行预测, $\left(\Delta x_{0}, \cdots, \Delta x_{7}, \Delta y_{0}, \cdots, \Delta y_{7}, \Delta z_{0}, \cdots, \Delta z_{7}\right)$。</p>
<h3 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h3><div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/1-4.png" style="width: 95%">
</div>

<h3 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h3><p>这篇论文当中融合了多个模态的信息，将点云处理为前视图和俯视图，设计了两个子网络分别进行3d候选框的提取以及多模态特征融合。本文的方法相比于只基于lidar和只基于图片的方法，在KITTI数据集的多个任务上都取得了很显著的提升，说明本文当中的融合多模态信息是非常有效的一种方法。</p>
<hr>
<h2 id="PointNet-Deep-Learning-on-Point-Sets-for-3D-Classification-and-Segmentation-CVPR-2017"><a href="#PointNet-Deep-Learning-on-Point-Sets-for-3D-Classification-and-Segmentation-CVPR-2017" class="headerlink" title="PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (CVPR 2017)"></a><a href="https://arxiv.org/abs/1612.00593" target="_blank" rel="noopener">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (CVPR 2017)</a></h2><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><p>传统的卷积架构要求输入是规则的数据格式，所以大多数之前的研究者都将点云数据转换到规则的3d体素网格或者多个视图的方法，然后再用卷积网络来做。但做这样的转换其实对数据的信息是有损的。</p>
<p>而在这篇paper当中提出了一种novel的网络结构，可以直接处理原始点云，保留输入点的信息。</p>
<p>本文工作的主要贡献包括：</p>
<ul>
<li>提出了一个新颖的网络结构，可以用来处理无序的3d点集。</li>
<li>阐明了怎么利用这个网络进行classification和segmentation的任务的。</li>
<li>对于模型的稳定性和鲁棒性提供了完整的实践和理论的分析。</li>
<li>阐明了网络提取的3d特征以及相对应的性能解释。</li>
</ul>
<h3 id="2-相关工作-1"><a href="#2-相关工作-1" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h3><p>这部分主要是介绍了三部分的前人工作：</p>
<ul>
<li>点云features</li>
<li>3D数据的学习方式</li>
<li>无序点集上的深度学习方式</li>
</ul>
<h3 id="3-问题阐述"><a href="#3-问题阐述" class="headerlink" title="3. 问题阐述"></a>3. 问题阐述</h3><p>点云可以用一系列的3D点来表示：${P_i | i = 1, …, n}, P_i = (x_i, y_i, z_i)$.这里只使用了三维坐标，也可以将颜色信息、intensity等加入点的表示当中。然后进行<code>classification</code>: 输出$k$ classes对应的得分。<code>segmentation</code>: 输出<code>nxm</code>个分数，相当于给每个点打label。</p>
<h3 id="4-PointNet网络"><a href="#4-PointNet网络" class="headerlink" title="4. PointNet网络"></a>4. PointNet网络</h3><h4 id="点集属性"><a href="#点集属性" class="headerlink" title="点集属性"></a>点集属性</h4><ul>
<li>无序，要求网络对于3d点集的任意组合都要invarient.</li>
<li>点之间的联系，点与点之间并不是隔离的，所以既需要capture局部特征，也需要capture全局特征。</li>
<li>转换不变性，输入点云是几何体，对于特定的转换应该保持不变性。</li>
</ul>
<h4 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h4><div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/2-1.png" style="width: 95%">
</div>

<p>网络的主要步骤如下：</p>
<ul>
<li>输入为点云中的n个点，所以输入为<code>nx3</code>的tensor，其中3代表的是三维坐标</li>
<li>输入经过一个T-Net，得到一个对齐矩阵，相当于做一个前处理</li>
<li>而后经过mlp，其中mlp参数是共享的，但是其实也是在每个点上操作，不同点之间计算feature的时候还是相对独立的(<code>所以这里肯定是能够改进的</code>)。</li>
<li>然后得到的features同样进过一个T-Net进行特征的对齐</li>
<li>再经过mlp扩到1024维，使用<code>max-pooling</code>对特征进行聚合，得到<code>1x1024</code>维的全局特征向量。</li>
<li>对于<strong>classification</strong>而言，后续步骤就是mlp+softmax得到分类结果</li>
<li>对于<strong>segmentation</strong>过程而言，需要融合local和global的特征，所以直接将全局特征concat到局部特征向量的每个维度，然后输出<code>nxm</code> scores.</li>
</ul>
<p>为了解决上述的点集的三个属性，作者在论文当中提出了对应的解决方法：</p>
<h5 id="对称函数"><a href="#对称函数" class="headerlink" title="对称函数"></a>对称函数</h5><p>对称函数是对于输入无关的函数，用来进行信息的聚合。在论文当中使用的是max-pooling进行全局信息的聚合，对于无序输入是invariant的。</p>
<p>作者希望通过运用对称函数，然后在点集上近似估计一个general的函数：</p>
<script type="math/tex; mode=display">
f\left(\left\{x_{1}, \ldots, x_{n}\right\}\right) \approx g\left(h\left(x_{1}\right), \ldots, h\left(x_{n}\right)\right)</script><p>其中$f: 2^{\mathbb{R}^{N}} \rightarrow \mathbb{R}, h: \mathbb{R}^{N} \rightarrow \mathbb{R}^{K}, g: \underbrace{\mathbb{R}^{K} \times \cdots \times \mathbb{R}^{K}}_{n} \rightarrow \mathbb{R}$</p>
<p>其中$h$函数相当于特征提取函数，$g$函数通过单变量函数和max-pooling函数组成。</p>
<h5 id="局部-amp-全局信息提取"><a href="#局部-amp-全局信息提取" class="headerlink" title="局部&amp;全局信息提取"></a>局部&amp;全局信息提取</h5><p>全局信息通过max-pooling后得到的<code>1x1024</code>维特征向量即为全局特征。而当进行segmentation任务时需要进行局部特征和全局特征的聚合，论文当中是直接将<code>1x1024</code>的全局特征按行concat到<code>nx64</code>维的局部特征当中，所以在segmentation网络的输入是<code>nx1088</code>维的特征。</p>
<h5 id="联合对齐网络"><a href="#联合对齐网络" class="headerlink" title="联合对齐网络"></a>联合对齐网络</h5><p>论文当中有两个T-Net，分别对输入的点云进行转换对齐，和计算得到的特征转换对齐。主要是解决点云的旋转不变性。T-Net是学习最有利于网络进行分类和分割的DxD旋转矩阵。</p>
<h4 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h4><p>这里主要着重阐述theorm 2:</p>
<p>Theorem 2. Suppose $\mathbf{u}: \mathcal{X} \rightarrow \mathbb{R}^{K}$ such that $\mathbf{u}: \underset{x_{i} \in S}{MAX\left(h\left(x_{i}\right)\right)}$ and $f=\gamma \circ \mathbf{u . T h e n ,}$<br>$(a) \forall S, \exists \mathcal{C}_{S}, \mathcal{N}_{S} \subseteq \mathcal{X}, f(T)=f(S)$ if $\mathcal{C}_{S} \subseteq T \subseteq \mathcal{N}_{S}$<br>$(b) \left|\mathcal{C}_{S}\right| \leq K$</p>
<p>对于这个公式的解释为：如果关键点集$\mathcal{C}_{S}$被全部保留，那么$F(S)$对于输入的干扰最后的分类结果是不变的。然后对于噪点的话，至多能到点集$\mathcal{N}_{S}$也是保持分类的结果不变的。也即最后的结果是由这样的关键点集决定的。</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/2-2.png" style="width: 95%">
</div>

<h3 id="5-不足"><a href="#5-不足" class="headerlink" title="5. 不足"></a>5. 不足</h3><p>在PointNet当中，计算点的特征基本都是独立计算的，aggression的话也只有max-pooling，其实是少了类似于CNN当中的capture范围内点特征的，点与点之间的联系在特征提取步骤体现的不多，后续肯定有工作改进这一点。</p>
<p>还有一个是输入的是$(x, y, z)$的三维坐标，感觉能提取的也就是空间信息，可以考虑加入其他的信息到input当中，提取的特征会更鲁棒一些，综合了其他的属性。</p>
<blockquote>
<p>Reference: <a href="https://zhuanlan.zhihu.com/p/73086704" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/73086704</a></p>
</blockquote>
<hr>
<h2 id="PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space-NIPS-2017"><a href="#PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space-NIPS-2017" class="headerlink" title="PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (NIPS 2017)"></a><a href="https://arxiv.org/abs/1706.02413" target="_blank" rel="noopener">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (NIPS 2017)</a></h2><h3 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h3><p>这篇工作主要是对PointNet进行改进，在PointNet当中提取的是点的独立的特征，不能捕捉到局部的结构特征。在PointNet++当中主要有两点改进。第一，利用度量空间距离，层级的利用PointNet提取特征，获得尺度越来越大的局部特征。第二，为了解决点云的密度分布不均匀的问题，作者在文章当中使用了MSG和MRG两种方式进行自适应的学习方法。</p>
<h3 id="2-介绍"><a href="#2-介绍" class="headerlink" title="2. 介绍"></a>2. 介绍</h3><p>类似于点云这种不规则的数据格式，需要对点集的任意组合都要做到invariant。之前的工作<code>PointNet</code>是针对点的空间位置的embedding，但是缺少对于局部结构特征的提取。不同于CNN，在每一层的卷积当中都会提取一定尺度感受野当中的局部特征。</p>
<p>在PointNet++当中需要解决两个问题：(1)怎么划分点云点集 (2)怎么通过特征学习器学习点云的局部特征。</p>
<p>在本文当中，使用了PointNet作为局部点集的特征提取器，并且层级的叠加使用PointNet进行特征提取，直到提取到最后全部点集的全局特征，这个步骤和CNN卷积过程比较类似。对于划分点集，作者采用的是领域球的方式，不同的领域之间会有一定的重叠。Centroids的选择可以通过FPS算法选取，半径的选择可以手动设定或者通过KNN来确定，两种方法对于最后的结果影响并不是很大。</p>
<h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h3><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/3-1.png" style="width: 95%">
</div>

<p>PointNet++网络是由一系列的<code>set abstraction levels</code>组成。在每一个level当中，是由以下的layers组成：</p>
<ul>
<li><p><strong>Sampling layer</strong><br>在该层当中，会采用FPS算法采样出N个中心点。</p>
</li>
<li><p><strong>Grouping layer</strong><br>在这一部分当中，主要是确定领域球的区域。可以手动选取半径，使得每个区域当中点的数量不超过$K$, 或者使用KNN的方法来划分区域。在每个区域当中点的数量可能是不一样的，不过PointNet可以处理为一个固定长度的特征向量，所以不太care这个东西。</p>
</li>
<li><p><strong>PointNet layer</strong><br>使用PointNet对输入区域的点集进行特征提取，输入的数据大小为$N^{‘} \times K \times (d + C)$. 输出的特征大小为$N^{‘} \times (d + C^{‘})$. 论文当中强调这个坐标值会先转换到局部region的一个相对坐标：$x_{i}^{(j)}=x_{i}^{(j)}-\hat{x}^{(j)}$.</p>
</li>
</ul>
<h4 id="非规整密度下的鲁棒特征学习"><a href="#非规整密度下的鲁棒特征学习" class="headerlink" title="非规整密度下的鲁棒特征学习"></a>非规整密度下的鲁棒特征学习</h4><p>这部分论文主要介绍了怎么在不同regions的密度分布不均匀的情况下进行鲁棒的特征学习。作者提出了两种方式来解决这个问题：</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/3-2.png" style="width: 55%">
</div>

<ul>
<li><p><strong>Multi-scale grouping(MSG)</strong><br>这种方法就是一种多尺度的学习方法。在grouping的时候选取不同的radius，即不同的空间scale，然后将PointNet当中得到的不同尺度的特征进行concat，但是这种方式最大的不足就是时间消耗很大。</p>
</li>
<li><p><strong>Multi-resolution grouping(MRG)</strong><br>对于level $L_i$的特征向量是由两部分的特征拼接而成。如图中所示，左边部分的vector是由底下的levels提取上来的局部特征。右边部分是对于全部在这些regions里面的原始点集运用PointNet得到的一个全局特征。</p>
</li>
</ul>
<h3 id="4-实验部分"><a href="#4-实验部分" class="headerlink" title="4. 实验部分"></a>4. 实验部分</h3><p>主要看下对于点云密度的方法对于实验性能的影响：</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/3-3.png" style="width: 95%">
</div>

<p>可以看到在实验当中，只使用SSG(single scale grouping)，在点集数量下降之后，性能下降很快。而加上MSG或者MRG+DP之后，性能下降就会大幅的减少，说明模型更加的鲁棒和稳定。不过这里并没有做单独的只有MSG和MRG的实验，所以说不定是DP的功劳呢。</p>
<h3 id="5-结论-1"><a href="#5-结论-1" class="headerlink" title="5. 结论"></a>5. 结论</h3><p>PointNet++对PointNet做了改进，通过层级的运用PointNet来提取局部的结构特征。并且提出了<code>MSG</code>和<code>MRG</code>两种方法来解决点云分布不均匀时的鲁棒特征学习。</p>
<hr>
<h2 id="VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-CVPR-2018"><a href="#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-CVPR-2018" class="headerlink" title="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection (CVPR 2018)"></a><a href="https://arxiv.org/abs/1711.06396" target="_blank" rel="noopener">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection (CVPR 2018)</a></h2><h3 id="1-简介-1"><a href="#1-简介-1" class="headerlink" title="1. 简介"></a>1. 简介</h3><ul>
<li>VoxelNet是一个端到端的，基于点云的3D检测方法。</li>
<li>提出和实现了一个高效的，在稀疏点云上进行并行处理的方法。</li>
<li>在KITTI上取得了SOTA的结果。</li>
</ul>
<h3 id="2-VoxelNet"><a href="#2-VoxelNet" class="headerlink" title="2. VoxelNet"></a>2. VoxelNet</h3><p>首先将点云划分为若干个voxel，然后对每个voxel当中的点集进行sample和归一化之后，通过多个<code>VFE(Voxel feature encoding)</code>层对voxel当中的点集提取局部特征。后面会介绍一下VFE层的实现。而后对得到的所有voxel features（4-Dimension）使用3D卷积进行特征的整合（增大感受野并且提取更加高层的feature）。而后接上RPN网络对anchor进行classification和regression。整体的pipeline如下图所示：</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/4-1.png" style="width: 95%">
</div>

<h4 id="特征提取网络"><a href="#特征提取网络" class="headerlink" title="特征提取网络"></a>特征提取网络</h4><p>主要是对voxel当中的点集进行特征提取，过程有点类似于PointNet的提取过程。分为四个过程：</p>
<h5 id="体素划分"><a href="#体素划分" class="headerlink" title="体素划分"></a>体素划分</h5><p>假设体素网格大小为$v_D, v_H, v_W$，那么我们可以得到3D voxel grid $D^{\prime}=D / v_{D}, H^{\prime}=H / v_{H}, W^{\prime}=W / v_{W}$.</p>
<h5 id="聚集-Grouping"><a href="#聚集-Grouping" class="headerlink" title="聚集(Grouping)"></a>聚集(Grouping)</h5><p>在一个voxel当中的点自动归为一个group，由于点云的稀疏性，所以每个voxel当中点的个数可能不尽相同。</p>
<h5 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h5><p>固定一个数$T$, 如果voxel当中点的个数超过了$T$，对其中的点随机采样$T$个点，这样做的好处一个是能节省计算资源，而是每次都是随机采样，增加了训练时的variation，使得最后的结果会更加鲁棒。</p>
<h5 id="层级特征提取"><a href="#层级特征提取" class="headerlink" title="层级特征提取"></a>层级特征提取</h5><p>首先看下每个点的表示方式，这里是用一个7维向量表示一个点。首先计算voxel当中所有点的mean值，记作$(v_x, v_y, v_z)$。则点的表示为:</p>
<script type="math/tex; mode=display">V_{in} = {(\hat{p}_i = [x_i, y_i, z_i, r_i, x_i-v_x,y_i-v_y,z_i-v_z]^T \in R^7)}</script><p>也即是图中的Point-wise Input，然后类似于在voxel当中使用PointNet。我们仔细看下，是通过MLP(FCN)对Input提取特征，使用Max-pooling得到一个局部的聚合特征，然后将这个聚合特征与point-wise特征进行聚合。这就是一个VFE layer，叠加多个VFE进行深度的特征提取。</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/4-2.png" style="width: 95%">
</div>

<h4 id="卷积中间层"><a href="#卷积中间层" class="headerlink" title="卷积中间层"></a>卷积中间层</h4><p>对得到的$C\times D^{\prime} \times H^{\prime} \times W^{\prime}$的四维特征矩阵运用3D卷积聚合voxel-wise特征，扩大感受野，提取更多的空间描述信息。</p>
<h4 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h4><p>RPN网络当中对设置的anchor进行classification和regression。如下图所示，输入是经过3D卷积之后得到的feature map。</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/4-3.png" style="width: 95%">
</div>

<p>这里解释一下最后概率score map第三个维度是2的原因，这是因为在每个位置上都设置了两个anchor，一个anchor是0度角，另外一个anchor是旋转了90度角，然后每个anchor都会回归3D box的七个参数，anchor的先验值是在训练集当中选出来的。所以在这个维度上分别是2和14.</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>回归目标：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\Delta x=\frac{x_{c}^{g}-x_{c}^{a}}{d^{a}}, \Delta y=\frac{y_{c}^{g}-y_{c}^{a}}{d^{a}}, \Delta z=\frac{z_{c}^{g}-z_{c}^{a}}{h^{a}} \\
\Delta l=\log \left(\frac{l^{g}}{l^{a}}\right), \Delta w=\log \left(\frac{w^{g}}{w^{a}}\right), \Delta h=\log \left(\frac{h^{g}}{h^{a}}\right) \\
\Delta \theta=\theta^{g}-\theta^{a}
\end{array}</script><p>损失函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L &=\alpha \frac{1}{N_{\text {pos }}} \sum_{i} L_{\text {cls }}\left(p_{i}^{\text {pos }}, 1\right)+\beta \frac{1}{N_{\text {neg }}} \sum_{j} L_{\text {cls }}\left(p_{j}^{\text {neg }}, 0\right) \\
&+\frac{1}{N_{\text {pos }}} \sum_{i} L_{\text {reg }}\left(\mathbf{u}_{i}, \mathbf{u}_{i}^{*}\right)
\end{aligned}</script><h3 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h3><div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/4-4.png" style="width: 95%">
</div>

<p>相比于之前的SOTA方法，VoxelNet的检测精度是最好的。</p>
<h3 id="4-一点思考"><a href="#4-一点思考" class="headerlink" title="4. 一点思考"></a>4. 一点思考</h3><p>VoxelNet直接针对点云进行检测，相比于其他Fusion的结果要更好，所以猜测是在Fusion当中直接将不同sensor的feature map结合在一起，不利于有用feature的学习。所以后续有一些工作在multi-sensor feature fusion这方面进行进一步的改进。</p>
<hr>
<h2 id="PointRCNN-3D-Object-Proposal-Generation-and-Detection-from-Point-Cloud-CVPR-2019"><a href="#PointRCNN-3D-Object-Proposal-Generation-and-Detection-from-Point-Cloud-CVPR-2019" class="headerlink" title="PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud (CVPR 2019)"></a><a href="https://arxiv.org/abs/1812.04244" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud (CVPR 2019)</a></h2><h3 id="1-简介-2"><a href="#1-简介-2" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>PointRCNN是第一个直接在原始点云上进行3D目标检测的网络，它是一个双阶段的网络。在第一个sub network生成一系列的高质量的3D region proposals，在第二个sub network进行进一步的refinement。</p>
<p>本文的主要贡献如下：<br>(1) 提出了一个bottom-up的基于点云的3D box proposal的生成算法，能够通过3D点云的segmentation来生成高质量的3D候选框。<br>(2) Box坐标的refinement，得益于标准坐标系转换(canonical coordinates)，并且提出了一个鲁棒的bin-based的损失函数。<br>(3) 在KITTI上取得了当时的SOTA效果。</p>
<h3 id="2-PointRCNN"><a href="#2-PointRCNN" class="headerlink" title="2. PointRCNN"></a>2. PointRCNN</h3><p>PointRCNN的整体框架pipeline如下所示：</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/5-1.png" style="width: 95%">
</div>

<p>上面的sub network生成3D的proposal，下面的sub network对生成的3D proposal在标准坐标系下进行refinement。</p>
<h4 id="3D-proposal-generation-via-segmentation"><a href="#3D-proposal-generation-via-segmentation" class="headerlink" title="3D proposal generation via segmentation"></a>3D proposal generation via segmentation</h4><p>因为作者发现，在3D点云当中，物体是自然分开的，并且是没有重叠的。所以3D detection label自然的就可以拿过来做segemntation的label，在GT box当中的点就视为前景点。</p>
<h5 id="Learning-point-cloud-representation"><a href="#Learning-point-cloud-representation" class="headerlink" title="Learning point cloud representation"></a>Learning point cloud representation</h5><p>在原始点云上使用PointNet++ with MSG提取点云特征，并且使用的是segmentation那路的方法，会得到一个point-wise的特征。</p>
<h5 id="Foreground-point-segmentation"><a href="#Foreground-point-segmentation" class="headerlink" title="Foreground point segmentation"></a>Foreground point segmentation</h5><p>前景分割和3D proposals的提取是并行处理的，在提取的point-wise特征之后接上一个segmentation head，对每个点进行label的预测。损失函数使用的是focal loss，因为正样本的比例相对会少很多。</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathcal{L}_{\text {focal }}\left(p_{t}\right)=-\alpha_{t}\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right) \\
\text { where } p_{t}=\left\{\begin{array}{ll}
p & \text { for forground point } \\
1-p & \text { otherwise }
\end{array}\right.
\end{array}</script><h5 id="Bin-based-generation"><a href="#Bin-based-generation" class="headerlink" title="Bin-based generation"></a>Bin-based generation</h5><p>这一块是论文当中比较创新的一个思路，将回归问题转化到bin的分类+res回归，实验结果显示相比于直接进行回归这种方式的效果会更好一些。</p>
<p>这里定义了一个search range $\mathcal{S}$. 这个search range相当于是一个查找范围，在这个范围上划分bin。还有一个uniform length $\delta$，是一个bin的长度。那么一个前景点$x^p$相对于GT点$x^{(p)}$就可以划分为两项，一项是bin值，还有一项是一个bin中的resual值。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{bin}_{x}^{(p)} &=\left\lfloor\frac{x^{p}-x^{(p)}+\mathcal{S}}{\delta}\right\rfloor, \operatorname{bin}_{z}^{(p)}=\left\lfloor\frac{z^{p}-z^{(p)}+\mathcal{S}}{\delta}\right\rfloor \\
\operatorname{res}_{u \in\{x, z\}}^{(p)} &=\frac{1}{\mathcal{C}}\left(u^{p}-u^{(p)}+\mathcal{S}-\left(\operatorname{bin}_{u}^{(p)} \cdot \delta+\frac{\delta}{2}\right)\right),(2) \\
\operatorname{res}_{y}^{(p)} &=y^{p}-y^{(p)}
\end{aligned}</script><p>这里在计算resual时是现将一个bin的中间值作为估计值，然后估计到中间值的resual。具体的关于bin的图示如下所示：</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/5-2.png" style="width: 95%">
</div>

<p>由于bin值被量化到了整数，所以在bin值上就是一个classification的问题，resual是一个regression的问题。由于3D框的$h,w,l$变化范围不大，所以直接进行regression。总的损失函数可以得到如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\mathcal{L}_{\text {bin }}^{(p)}=\sum_{u \in\{x, z, \theta\}}\left(\mathcal{F}_{\text {cls }}\left(\widehat{\operatorname{bin}}_{u}^{(p)}, \operatorname{bin}_{u}^{(p)}\right)+\mathcal{F}_{\text {reg }}\left(\widehat{\text { res }}_{u}^{(p)}, \operatorname{res}_{u}^{(p)}\right)\right)\\
&\mathcal{L}_{\mathrm{res}}^{(p)}=\sum_{v \in\{y, h, w, l\}} \mathcal{F}_{\mathrm{reg}}\left(\widehat{\operatorname{res}}_{v}^{(p)}, \operatorname{res}_{v}^{(p)}\right)\\
&\mathcal{L}_{\mathrm{reg}}=\frac{1}{N_{\mathrm{pos}}} \sum_{p \in \mathrm{pos}}\left(\mathcal{L}_{\mathrm{bin}}^{(p)}+\mathcal{L}_{\mathrm{res}}^{(p)}\right)
\end{aligned}</script><h4 id="Point-cloud-region-pooling"><a href="#Point-cloud-region-pooling" class="headerlink" title="Point cloud region pooling"></a>Point cloud region pooling</h4><p>在上一步生成3D ROI之后，会先对ROI enlarge一部分，enlarge的值记为$\eta$. 也即是在长宽高三个维度上enlarge，每个点的特征包括点的3D坐标$x, y, z$, 反射率$r$，以及之前生成的segmentation mask $m$. 还有之前得到的$C$维的point-wise feature。</p>
<h4 id="Canonical-3D-box-refinement"><a href="#Canonical-3D-box-refinement" class="headerlink" title="Canonical 3D box refinement"></a>Canonical 3D box refinement</h4><p>在这一步首先会对3D box进行一个标准坐标系转换，如下图所示，就是从lidar坐标系转换到一个自身相关的标准坐标系：</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/5-3.png" style="width: 95%">
</div>

<p>在进行坐标系转换之后，每个点的深度信息就消失了，所以作者添加了:</p>
<script type="math/tex; mode=display">
d^{(p)} = \sqrt{(x^{(p)})^2 + (y^{(p)})^2 + (z^{(p)})^2}</script><p>到点的特征当中。</p>
<p>这个时候refinement的损失函数可以看到，首先看回归目标：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\tilde{\mathbf{b}}_{i}=\left(0,0,0, h_{i}, w_{i}, l_{i}, 0\right) \\
\tilde{\mathbf{b}}_{i}^{\mathrm{gt}}=\left(x_{i}^{\mathrm{gt}}-x_{i}, y_{i}^{\mathrm{gt}}-y_{i}, z_{i}^{\mathrm{gt}}-z_{i}, h_{i}^{\mathrm{gt}}, w_{i}^{\mathrm{gt}}, l_{i}^{\mathrm{gt}}, \theta_{i}^{\mathrm{gt}}-\theta_{i}\right)
\end{array}</script><p>然后对于旋转角度：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\operatorname{bin}_{\Delta \theta}^{i}=\left[\frac{\theta_{i}^{\mathrm{gt}}-\theta_{i}+\frac{\pi}{4}}{\omega}\right] \\
\operatorname{res}_{\Delta \theta}^{i}=\frac{2}{\omega}\left(\theta_{i}^{\mathrm{gt}}-\theta_{i}+\frac{\pi}{4}-\left(\operatorname{bin}_{\Delta \theta}^{i} \cdot \omega+\frac{\omega}{2}\right)\right)
\end{array}</script><p>所以第二阶段的损失函数总结为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}_{\text {refine }}=& \frac{1}{\|\mathcal{B}\|} \sum_{i \in \mathcal{B}} \mathcal{F}_{\text {cls }}\left(\text { prob }_{i}, \text { label }_{i}\right) \\
&+\frac{1}{\left\|\mathcal{B}_{\text {pos }}\right\|} \sum_{i \in \mathcal{B}_{\text {pos }}}\left(\tilde{\mathcal{L}}_{\text {bin }}^{(i)}+\tilde{\mathcal{L}}_{\text {res }}^{(i)}\right)
\end{aligned}</script><h3 id="3-实验-1"><a href="#3-实验-1" class="headerlink" title="3. 实验"></a>3. 实验</h3><div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/5-4.png" style="width: 95%">
</div>

<p>在训练时先训练3D proposal网络，然后训练refinement网络，所以是一个双阶段的检测网络。</p>
<hr>
<h2 id="PI-RCNN-An-Efficient-Multi-sensor-3D-Object-Detector-with-Point-based-Attentive-Cont-conv-Fusion-Module-AAAI-2020"><a href="#PI-RCNN-An-Efficient-Multi-sensor-3D-Object-Detector-with-Point-based-Attentive-Cont-conv-Fusion-Module-AAAI-2020" class="headerlink" title="PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module (AAAI 2020)"></a><a href="https://arxiv.org/abs/1911.06084" target="_blank" rel="noopener">PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module (AAAI 2020)</a></h2><p>实验室的一篇paper，主要探究的是如何有效的将image feature和lidar point cloud进行融合的工作。</p>
<h3 id="1-简介-3"><a href="#1-简介-3" class="headerlink" title="1. 简介"></a>1. 简介</h3><h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>Direct fusion的方式忽略了RGB图片和Bird-view的不同视角的差异，而基于Frustum的方式依赖于2D检测器的精度。Continuous convolution在BEV上和image feature进行融合，但是将点云投影到BEV上会有量化和最近邻搜索的误差，所以该paper将image feature直接与原始点云进行融合。</p>
<h4 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h4><ul>
<li>提出了PACF模块融合图像和点云的特征，包含point-wise continuous convolution、point-pooling和attentive aggregation三个组件。</li>
<li>结合了多任务学习，包括图像segmentation和3d detection任务。</li>
<li>在KITTI上的实验说明了方法的有效性。</li>
</ul>
<h3 id="2-PI-RCNN"><a href="#2-PI-RCNN" class="headerlink" title="2. PI-RCNN"></a>2. PI-RCNN</h3><h4 id="网络总体架构"><a href="#网络总体架构" class="headerlink" title="网络总体架构"></a>网络总体架构</h4><div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/6-1.png" style="width: 95%">
</div>

<ol>
<li>image segmentation网络获取图像的semantic feature</li>
<li>检测子网络1提取3D proposals</li>
<li>PACF module将图像feature和proposal当中的点进行融合</li>
<li>检测子网络2进行3d detection任务</li>
</ol>
<h4 id="PACF-module"><a href="#PACF-module" class="headerlink" title="PACF module"></a>PACF module</h4><p>这个模块是本文最主要的创新点，PACF module的主要工作机如下图所示：</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/6-2.png" style="width: 95%">
</div>

<ol>
<li>对region当中的每个点搜索k个邻点，可以使用knn方法搜索</li>
<li>根据lidar-camera标定参数投影到图像坐标系</li>
<li>提取该k个点对应的image-feature，与该k点的point-wise feature进行concat，得到$f_i$是$(C_{seg} + C_{lidar})$维度的特征。</li>
<li>然后是3个组件的相关表示：</li>
</ol>
<p><strong>Point-wise continuous convolution</strong></p>
<script type="math/tex; mode=display">
\begin{array}{c}
y_{c c, k}^{i}=\operatorname{MLP}_{c c}\left(f_{k}^{\prime}\right), \quad f_{k}^{\prime}=\operatorname{CONCAT}\left(f_{k}, x_{k}-x_{i}\right) \\
y_{c c}^{i}=\sum_{k} y_{c c, k}^{i}
\end{array}</script><p>这里MLP近似continuous convolution操作。</p>
<p><strong>point-pooling</strong></p>
<script type="math/tex; mode=display">
y_{\text {pool}}^{i}=\operatorname{POOL}\left(F^{\prime}\right), \quad F^{\prime}=\left[f_{1}^{\prime T}, f_{2}^{\prime T}, \ldots, f_{K}^{\prime T}\right]^{T}</script><p><strong>Attentive aggregation</strong></p>
<script type="math/tex; mode=display">
y_{a}^{i}=\operatorname{MLP}_{a g g r}\left(Y_{c c}^{i}\right)=\sum_{k} w_{k} y_{c c, k}^{i}</script><p><strong>Concat</strong></p>
<script type="math/tex; mode=display">
y_{o}^{i}=\operatorname{CONCAT}\left(y_{c c}^{i}, y_{a}^{i}, y_{\text {pool}}^{i}\right)</script><h3 id="3-实验-2"><a href="#3-实验-2" class="headerlink" title="3. 实验"></a>3. 实验</h3><p>在具体实现当中，image segmentation选择的轻量级的U-Net，3D检测框架使用的是PointRCNN。</p>
<div align="center">
<img src="/2020/08/19/3d-detection-paper-notes/6-3.png" style="width: 65%">
</div>

<h3 id="4-一点思考-1"><a href="#4-一点思考-1" class="headerlink" title="4. 一点思考"></a>4. 一点思考</h3><p>反观现在KITTI上排前的方法基本都是lidar-based方法，但其实我们知道multi-sensors能做到的精度肯定是要比lidar-based方法要高的。个人有这几点小看法：(1) image主要是蕴含很丰富的语义信息，点云当中蕴含更多的是结构信息，这两种不同feature的fusion机制相对而言会比这样concat然后做fusion的机制要更高级。(2) image当中的特征在3D proposal的提取过程当中并木有用到，但是这是一个可以改进的地方，可不可以找到一种方式能够提高proposal的recall值。(3) 看的这些文章，大部分的框架都是差不多的，大家一般是在<strong>特征</strong>这方面进行了很多种探索，哎任重而道远啊，还是得多思考。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/3d-detection/" rel="tag"><i class="fa fa-tag"></i> 3d-detection</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/08/06/leetcode-solutions/" rel="next" title="leetcode-solutions">
                <i class="fa fa-chevron-left"></i> leetcode-solutions
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/09/18/%E5%85%AC%E5%85%B1%E7%BB%8F%E6%B5%8E%E5%AD%A6/" rel="prev" title="公共经济学">
                公共经济学 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars2.githubusercontent.com/u/28039767?s=400&u=43e653f3e1abe2fa56182133d26fc581ed4b2b40&v=4"
                alt="huangyifei" />
            
              <p class="site-author-name" itemprop="name">huangyifei</p>
              <p class="site-description motion-element" itemprop="description">Master student majored in computer science of zhejiang university.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/MySuperSoul" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:huangyifei0910@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-View-3D-Object-Detection-Network-for-Autonomous-Driving-CVPR-2017"><span class="nav-number">1.</span> <span class="nav-text">Multi-View 3D Object Detection Network for Autonomous Driving (CVPR 2017)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-简介"><span class="nav-number">1.1.</span> <span class="nav-text">1. 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-相关工作"><span class="nav-number">1.2.</span> <span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-MV3D网络"><span class="nav-number">1.3.</span> <span class="nav-text">3. MV3D网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3D点云表示"><span class="nav-number">1.3.1.</span> <span class="nav-text">3D点云表示</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Bird-eye-view"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">Bird eye view</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Front-View"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">Front View</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Images"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">Images</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3D-proposal-network"><span class="nav-number">1.3.2.</span> <span class="nav-text">3D proposal network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Region-based-Fusion-network"><span class="nav-number">1.3.3.</span> <span class="nav-text">Region-based Fusion network</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Deep-fusion"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">Deep fusion</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Oriented-3d-box-regression"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">Oriented 3d box regression</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-实验结果"><span class="nav-number">1.4.</span> <span class="nav-text">4. 实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-结论"><span class="nav-number">1.5.</span> <span class="nav-text">5. 结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PointNet-Deep-Learning-on-Point-Sets-for-3D-Classification-and-Segmentation-CVPR-2017"><span class="nav-number">2.</span> <span class="nav-text">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (CVPR 2017)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-介绍"><span class="nav-number">2.1.</span> <span class="nav-text">1. 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-相关工作-1"><span class="nav-number">2.2.</span> <span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-问题阐述"><span class="nav-number">2.3.</span> <span class="nav-text">3. 问题阐述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-PointNet网络"><span class="nav-number">2.4.</span> <span class="nav-text">4. PointNet网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#点集属性"><span class="nav-number">2.4.1.</span> <span class="nav-text">点集属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网络架构"><span class="nav-number">2.4.2.</span> <span class="nav-text">网络架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#对称函数"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">对称函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#局部-amp-全局信息提取"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">局部&amp;全局信息提取</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#联合对齐网络"><span class="nav-number">2.4.2.3.</span> <span class="nav-text">联合对齐网络</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#理论分析"><span class="nav-number">2.4.3.</span> <span class="nav-text">理论分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-不足"><span class="nav-number">2.5.</span> <span class="nav-text">5. 不足</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space-NIPS-2017"><span class="nav-number">3.</span> <span class="nav-text">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (NIPS 2017)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-摘要"><span class="nav-number">3.1.</span> <span class="nav-text">1. 摘要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-介绍"><span class="nav-number">3.2.</span> <span class="nav-text">2. 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-方法"><span class="nav-number">3.3.</span> <span class="nav-text">3. 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#网络结构"><span class="nav-number">3.3.1.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#非规整密度下的鲁棒特征学习"><span class="nav-number">3.3.2.</span> <span class="nav-text">非规整密度下的鲁棒特征学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-实验部分"><span class="nav-number">3.4.</span> <span class="nav-text">4. 实验部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-结论-1"><span class="nav-number">3.5.</span> <span class="nav-text">5. 结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-CVPR-2018"><span class="nav-number">4.</span> <span class="nav-text">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection (CVPR 2018)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-简介-1"><span class="nav-number">4.1.</span> <span class="nav-text">1. 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-VoxelNet"><span class="nav-number">4.2.</span> <span class="nav-text">2. VoxelNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#特征提取网络"><span class="nav-number">4.2.1.</span> <span class="nav-text">特征提取网络</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#体素划分"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">体素划分</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#聚集-Grouping"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">聚集(Grouping)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机采样"><span class="nav-number">4.2.1.3.</span> <span class="nav-text">随机采样</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#层级特征提取"><span class="nav-number">4.2.1.4.</span> <span class="nav-text">层级特征提取</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积中间层"><span class="nav-number">4.2.2.</span> <span class="nav-text">卷积中间层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RPN"><span class="nav-number">4.2.3.</span> <span class="nav-text">RPN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-number">4.2.4.</span> <span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-实验"><span class="nav-number">4.3.</span> <span class="nav-text">3. 实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-一点思考"><span class="nav-number">4.4.</span> <span class="nav-text">4. 一点思考</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PointRCNN-3D-Object-Proposal-Generation-and-Detection-from-Point-Cloud-CVPR-2019"><span class="nav-number">5.</span> <span class="nav-text">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud (CVPR 2019)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-简介-2"><span class="nav-number">5.1.</span> <span class="nav-text">1. 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-PointRCNN"><span class="nav-number">5.2.</span> <span class="nav-text">2. PointRCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3D-proposal-generation-via-segmentation"><span class="nav-number">5.2.1.</span> <span class="nav-text">3D proposal generation via segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-point-cloud-representation"><span class="nav-number">5.2.1.1.</span> <span class="nav-text">Learning point cloud representation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Foreground-point-segmentation"><span class="nav-number">5.2.1.2.</span> <span class="nav-text">Foreground point segmentation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Bin-based-generation"><span class="nav-number">5.2.1.3.</span> <span class="nav-text">Bin-based generation</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Point-cloud-region-pooling"><span class="nav-number">5.2.2.</span> <span class="nav-text">Point cloud region pooling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Canonical-3D-box-refinement"><span class="nav-number">5.2.3.</span> <span class="nav-text">Canonical 3D box refinement</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-实验-1"><span class="nav-number">5.3.</span> <span class="nav-text">3. 实验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PI-RCNN-An-Efficient-Multi-sensor-3D-Object-Detector-with-Point-based-Attentive-Cont-conv-Fusion-Module-AAAI-2020"><span class="nav-number">6.</span> <span class="nav-text">PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module (AAAI 2020)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-简介-3"><span class="nav-number">6.1.</span> <span class="nav-text">1. 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivation"><span class="nav-number">6.1.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Contributions"><span class="nav-number">6.1.2.</span> <span class="nav-text">Contributions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-PI-RCNN"><span class="nav-number">6.2.</span> <span class="nav-text">2. PI-RCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#网络总体架构"><span class="nav-number">6.2.1.</span> <span class="nav-text">网络总体架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PACF-module"><span class="nav-number">6.2.2.</span> <span class="nav-text">PACF module</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-实验-2"><span class="nav-number">6.3.</span> <span class="nav-text">3. 实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-一点思考-1"><span class="nav-number">6.4.</span> <span class="nav-text">4. 一点思考</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2019/06/17 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">huangyifei</span>

  
</div>


  <div class="powered-by">
  <i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
    本站访客数:<span id="busuanzi_value_site_uv"></span>
  </span>
</div>
  <!-- <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div> -->



  <span class="post-meta-divider">|</span>



  <!-- <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div> -->




<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共46.4k字</span>
</div>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/custom/custom.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '5pTlbsRYAO0H1vuT7jkHu7Ut-gzGzoHsz',
        appKey: 'yfSXCC53lgKMxeYS3UtderOT',
        placeholder: 'Comment here!',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("5pTlbsRYAO0H1vuT7jkHu7Ut-gzGzoHsz", "yfSXCC53lgKMxeYS3UtderOT");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":150,"height":225},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
